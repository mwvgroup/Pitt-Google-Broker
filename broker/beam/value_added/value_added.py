#!/usr/bin/env python3
# -*- coding: UTF-8 -*-

"""``value_added`` runs a Beam pipeline to processes alerts. Currently it:

1. Listens to the Pub/Sub stream
generated by the Consumer (which runs on a Compute Engine VM,
separate from this Beam pipeline).

2. Extracts the alert data as a dict and drops the cutouts.

3. Filters for likely extragalactic transients; publishes a Pub/Sub
stream with alerts that pass the filter.

4. Attempts to fit likely extragalactic transients using Salt2.
When the fit is successful:
stores a figure (object's lightcurve + Salt2 fit) in a Cloud Storage bucket;
publishes a Pub/Sub stream with fit results;
loads fit results to a BigQuery table.

Usage Example
-------------

.. code-block:: python
   :linenos:

Module Documentation
--------------------
"""

import argparse
import logging
import apache_beam as beam
from apache_beam.io import BigQueryDisposition as bqdisp
from apache_beam.io import ReadFromPubSub, WriteToPubSub, WriteToBigQuery
from apache_beam.io.gcp.bigquery_tools import RetryStrategy
from apache_beam.options.pipeline_options import PipelineOptions

from broker_utils import beam_transforms as bbt
from broker_utils import schema_maps as bsm

from transforms import filters as tf
from transforms import salt2 as ts2



def load_schema_map(SURVEY, TESTID):
    # load the schema map from the broker bucket in Cloud Storage
    return bsm.load_schema_map(SURVEY, TESTID)

def sink_configs(PROJECTID, SURVEY):
    """Configuration dicts for all pipeline sinks.

    Args:
        PROJECTID (str): Google Cloud Platform project ID
        SURVEY (str):    Survey this pipeline will process.

    Returns:
        sink_configs = {'sinkResource_dataDescription': {'config_name': value, }, }
    """
    sink_configs = {
            'BQ_salt2': {
                'create_disposition': bqdisp.CREATE_NEVER,
                'write_disposition': bqdisp.WRITE_APPEND,
                'insert_retry_strategy': RetryStrategy.RETRY_ON_TRANSIENT_ERROR,
            },
            'PS_generic': {
                'with_attributes': True,
                #  may want to use these in the future:
                # 'id_label': None,
                'timestamp_attribute': 'publish_time'
            },
    }

    if SURVEY == 'decat':
        sink_configs['BQ_salt2']['insert_retry_strategy'] = RetryStrategy.RETRY_NEVER

    return sink_configs


class Salt2(beam.PTransform):
    """ Composite PTransform of all Salt2-related transforms:
    1. Extract epochs
    2. Filter out alerts that do not meet minimum data quality
    3. Perform Salt2 fit
    4. Store a lightcurve + Salt2 fit figure in Cloud Storage
    5. Store fit results in BigQuery
    6. Publish fit results to Pub/Sub
    """
    def __init__(self, schema_map, sinks, sink_configs, salt2_configs):
        super().__init__()
        self.schema_map = schema_map
        self.sinks = sinks
        self.sink_configs = sink_configs
        self.salt2_configs = salt2_configs

    def expand(self, alert_PColl):
        """
        Args:
            alert_PColl (PCollection[dict,]): PCollection of dictionaries
            of alert data
        """
        schema_map = self.schema_map
        sinks = self.sinks
        sink_configs = self.sink_configs
        s2conf = self.salt2_configs

        # extract the epochs and some stats
        alert_epoch_dicts = (
            alert_PColl | 'FormatForSalt2' >>
            beam.ParDo(ts2.FormatForSalt2(schema_map, s2conf))
        )

        # drop alerts that do not meet minimum data quality
        alert_epoch_dicts_QC = (
            alert_epoch_dicts | 'filterSalt2QualityCuts' >>
            beam.Filter(ts2.salt2_quality_cuts, s2conf)
        )

        # fit with Salt2. Yields 2 output collections
        salt2Dicts = (
            alert_epoch_dicts_QC | 'FitSalt2' >>
            beam.ParDo(ts2.FitSalt2(schema_map)).with_outputs('salt2Fit4Figure', 'alert_salt2Fit', main='salt2Fit')
        )
        salt2Fit = salt2Dicts.salt2Fit  # PCollection of dicts
        alert_salt2Fit = salt2Dicts.alert_salt2Fit  # PCollection of dicts
        salt2Fit4Figure = salt2Dicts.salt2Fit4Figure  # PCollection of dicts

        # Store a lightcurve + Salt2 fit figure in Cloud Storage
        __ = (
            salt2Fit4Figure | 'StoreSalt2FitFigure' >>
            beam.ParDo(ts2.StoreSalt2FitFigure(sinks['CS_salt2']))
        )

        # Store the fit params in BigQuery
        bqSalt2Deadletters = (
            salt2Fit | 'salt2ToBQ' >>
            WriteToBigQuery(sinks['BQ_salt2'], **sink_configs['BQ_salt2'])
        )  # ToDo: handle deadletters

        # Announce the fit params to Pub/Sub and include original alert
        salt2PS = (
            alert_salt2Fit | 'salt2FormatDictForPubSub' >>
            beam.ParDo(bbt.FormatDictForPubSub())
        )
        psSalt2Deadletters = (
            salt2PS | 'salt2ToPubSub' >>
            WriteToPubSub(sinks['PS_salt2'], **sink_configs['PS_generic'])
        )  # ToDo: handle deadletters

        return salt2Fit


def run(schema_map, sources, sinks, sink_configs, pipeline_args, salt2_configs):
    """Runs the Beam pipeline.
    """
    pipeline_options = PipelineOptions(pipeline_args, streaming=True)

    with beam.Pipeline(options=pipeline_options) as pipeline:

        #-- Read from PS and extract data as dicts
        alert_bytes = (
            pipeline | 'ReadFromPubSub' >>
            ReadFromPubSub(topic=sources['PS_alerts'])
        )
        full_alert_dicts = (
            alert_bytes | 'ExtractAlertDict' >>
            beam.ParDo(bbt.ExtractAlertDict())
        )
        alert_dicts = (
            full_alert_dicts | 'StripCutouts' >>
            beam.ParDo(bbt.StripCutouts(schema_map))
        )

        #-- Filter for purity and publish a Pub/Sub stream
        adicts_pure = (
            alert_dicts | 'filterPurity' >>
            beam.Filter(tf.is_pure, schema_map)
        )
        adicts_pure_ps = (
            adicts_pure | 'pureTransFormatDictForPubSub' >>
            beam.ParDo(bbt.FormatDictForPubSub())
        )
        adicts_pure_ps_deadletters = (
            adicts_pure_ps | 'pureTransToPubSub' >>
            WriteToPubSub(sinks['PS_pure'], **sink_configs['PS_generic'])
        )  # ToDo: handle deadletters

        #-- Filter for extragalactic transients and publish a Pub/Sub stream
        adicts_exgal = (
            alert_dicts | 'filterExgalTrans' >>
            beam.Filter(tf.is_extragalactic_transient, schema_map)
        )
        adicts_exgal_ps = (
            adicts_exgal | 'exgalTransFormatDictForPubSub' >>
            beam.ParDo(bbt.FormatDictForPubSub())
        )
        adicts_exgal_ps_deadletters = (
            adicts_exgal_ps | 'exgalTransToPubSub' >>
            WriteToPubSub(sinks['PS_exgalTrans'], **sink_configs['PS_generic'])
       )  # ToDo: handle deadletters

        #-- Fit with Salt2, store and announce results
        __ = (
            adicts_exgal | 'Salt2' >>
            Salt2(schema_map, sinks, sink_configs, salt2_configs)
        )



if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--PROJECTID",
        help="Google Cloud Platform project name.\n",
    )
    parser.add_argument(
        "--SURVEY",
        help="Survey this broker instance is associated with.\n",
    )
    parser.add_argument(
        "--TESTID",
        help="testid this broker instance is associated with.\n",
    )
    parser.add_argument(
        "--source_PS_alerts",
        help="Pub/Sub topic to read alerts from.\n"
        '"projects/<PROJECT_NAME>/topics/<TOPIC_NAME>".',
    )
    parser.add_argument(
        "--sink_BQ_salt2",
        help="BigQuery table to store Salt2 fits.\n",
    )
    parser.add_argument(
        "--sink_CS_salt2",
        help="Cloud Storage bucket to store Salt2 figure.\n",
    )
    parser.add_argument(
        "--sink_PS_pure",
        help="Pub/Sub topic to announce alert stream filtered for purity.\n",
    )
    parser.add_argument(
        "--sink_PS_exgalTrans",
        help="Pub/Sub topic to announce extragalactic transient filter.\n",
    )
    parser.add_argument(
        "--sink_PS_salt2",
        help="Pub/Sub topic to announce Salt2 fit.\n",
    )
    parser.add_argument(
        "--salt2_SNthresh",
        default=5.,
        help="S/N threshold. Minimum S/N necessary to fit Salt2. Also, Salt2 param t0 constrained around first epoch with S/N > salt2_SNthresh.\n",
    )
    parser.add_argument(
        "--salt2_minNdetections",
        default=5,
        help="Minimum number of detections necessary to fit Salt2.\n",
    )
    # parser.add_argument(
    #     "--vizierCat",
    #     default="vizier:II/246/out",
    #     help="Vizier catalog to cross-match against."
    # )

    known_args, pipeline_args = parser.parse_known_args()

    schema_map = load_schema_map(known_args.SURVEY, known_args.TESTID)
    sources = {'PS_alerts': known_args.source_PS_alerts}
    sinks = {
            'BQ_salt2': known_args.sink_BQ_salt2,
            'CS_salt2': known_args.sink_CS_salt2,
            'PS_pure': known_args.sink_PS_pure,
            'PS_exgalTrans': known_args.sink_PS_exgalTrans,
            'PS_salt2': known_args.sink_PS_salt2,
    }
    sink_configs = sink_configs(known_args.PROJECTID, known_args.SURVEY)
    salt2_configs = {
                    'SNthresh': float(known_args.salt2_SNthresh),
                    'minNdetections': int(known_args.salt2_minNdetections),
    }

    run(schema_map, sources, sinks, sink_configs, pipeline_args, salt2_configs)

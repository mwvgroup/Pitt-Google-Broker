#! /bin/bash

PROJECT_ID=$1
# PROJECT_ID=$GOOGLE_CLOUD_PROJECT
testid=$2
beamdir=$3

# sources and sinks
source_PS_ztf="projects/${PROJECT_ID}/topics/ztf_alert_data"
sink_BQ_originalAlert=ztf_alerts.alerts
sink_BQ_salt2=ztf_alerts.salt2
sink_CS_salt2=${PROJECT_ID}_ztf-sncosmo
sink_PS_exgalTrans=projects/${PROJECT_ID}/topics/ztf_exgalac_trans
sink_PS_salt2=projects/${PROJECT_ID}/topics/ztf_salt2
if [ "$testid" != "False" ]; then
    source_PS_ztf="${source_PS_ztf}-${testid}"
    sink_BQ_originalAlert="ztf_alerts_${testid}.alerts"
    sink_BQ_salt2="ztf_alerts_${testid}.salt2"
    sink_CS_salt2="${sink_CS_salt2}-${testid}"
    sink_PS_exgalTrans="${sink_PS_exgalTrans}-${testid}"
    sink_PS_salt2="${sink_PS_salt2}-${testid}"
fi

# job-specific configs
dataflow_job_name_bqsink=bq-sink
dataflow_job_name_valadd=value-added
max_num_workers_bqsink=20
max_num_workers_valadd=10
setup_file_bqsink=${beamdir}/ztf_bq_sink/setup.py
setup_file_valadd=${beamdir}/ztf_value_added/setup.py
if [ "$testid" != "False" ]; then
    dataflow_job_name_bqsink="${dataflow_job_name_bqsink}-${testid}"
    dataflow_job_name_valadd="${dataflow_job_name_valadd}-${testid}"
fi

# generic job configs
region=us-central1
runner=DataflowRunner
beam_bucket=${PROJECT_ID}_dataflow
if [ "$testid" != "False" ]; then
    beam_bucket="${beam_bucket}-${testid}"
fi
staging_location=gs://${beam_bucket}/staging
temp_location=gs://${beam_bucket}/temp

# Salt2 configs
salt2_SNthresh=5.
salt2_minNdetections=5

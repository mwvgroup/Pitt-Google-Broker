# Creating and Using a Broker Instance for Testing

The following instructions will walk you through deploying an instance of the v0.3 broker code to GCP so that you can test the broker and its components.

See the repo's main [../README.md](../README.md) for a description of the broker architecture and links to the production resources.

Table of Contents:
- [Overview](#overview)
    - [Setup and utilize a _testing_ instance of the broker](#setup-and-utilize-a-testing-instance-of-the-broker)
    - [Setup a _production_ instance of the broker](#setup-a-production-instance-of-the-broker)
- [Detailed Workflow with code examples](#detailed-workflow-with-code-examples)
    - [1. Setup a testing instance](#1-setup-a-testing-instance)
    - [2. Run your tests](#2-run-your-tests)
        - [Example: Run the broker](#example-run-the-broker).
            - [End-to-End](#end-to-end). In this section you can:
                - Connect to ZTF's _live stream_
                - Connect to a ZTF _stream from a previous night_ (this will _flood_ the broker with alerts)
            - [Use the consumer simulator](#use-the-consumer-simulator) (this bypasses the broker's consumer, and is the only option by which you can _control the flow_ of alerts into the broker)
    - [3. Option a: Leave the testing instance inactive](#3a-leave-the-testing-instance-inactive)
    - [3. Option b: Teardown the testing instance](#3b-teardown-the-testing-instance)

Other docs:
- [Setup a broker instance](setup-broker.md)
- [Where to view broker instance resources](view-resources.md)
- [Run a broker instance](run-broker.md)

# Overview

## Setup and utilize a _testing_ instance of the broker
<!-- fs -->
In this tutorial, we will create and deploy a _testing_ instance of the broker which you can play around with without touching any of the GCP resources or code that is running nightly in _production_.
We do this by creating new GCP resources (VMs, Pub/Sub streams, buckets, etc.) from the source code in this branch, which have their names tagged with a user-defined string, the "testid".
Switches have been placed throughout the code which connect resources that are tagged with the same testid.
For example, _if we create and deploy a testing instance of the broker with the testid "mytest":_
- the `ztf-consumer` VM will be named `ztf-consumer-mytest`,
    - it will publish alerts to the Pub/Sub topic named `ztf_alerts-mytest`
- the downstream components (e.g., Dataflow jobs) will have similarly tagged names,
    - they will get their input data from the `ztf_alerts-mytest` topic,
    - and they will publish to resources with similarly tagged names.

You can use this testing instance to test/alter/play around with any component of the broker.
In this tutorial we will use it to test the broker setup scripts, the nightly deployment scripts, and the consume/process pipelines.
In the future, we can use testing instances to:
- debug (e.g., alter the startup script used by the `ztf-consumer-{testid}` VM and test the effect.)
- run unit tests (e.g., I anticipate that we should configure Travis to build/use a testing instance with the testid "travis")
- develop and test a new science component (e.g., ML classifier) for the value-added pipeline.

I think it should be straightforward to configure a _testing_ instance to connect to some component(s) of the _production_ broker (e.g., connecting a testing instance of a Dataflow job to the live `ztf_alerts` Pub/Sub stream generated by the production consumer)
by altering or simply removing the relevant switch(es) that tag the name(s) of testing resource(s) with the testid.

If you are not actively using your testing instance, but you are not ready to delete it,
be sure to _shutdown all VMs and stop/drain all Dataflow jobs_ (these are the most expensive components to leave running when not in use).
See the specific code below, but in general the simplest way to do this to trigger the `night-conductor` VM to end the night.

When you are done with it, please teardown (delete) your testing instance of the broker.
See below for the specific code, but essentially you execute the `setup_broker.sh` script with the `teardown` flag set to `True`.

#### Requirements on `testid`
- must be a short _string_ (this gets appended to resource names, which have length limits)
- must _start with_ a lowercase letter
- _may_ contain __*lowercase letters and numbers*__
- may _not_ contain capital letters
- may _not_ contain any other characters including spaces, "-", "_", "/", etc.

<!-- fe Create, deploy, and utilize a testing instance of the broker -->

## Setup a _production_ instance of the broker
<!-- fs -->
Strictly speaking, this tutorial is not intended to cover this topic, but the only difference between a testing and production instance of the broker is the presence/absence of a tag (string) appended to the names of all GCP resources that created for, and used by, the broker instance.
So, I will cover some relevant items here.

The production instance only needs to be _setup_ once and it has already been done.

Most components of the broker can be _updated_ by simply replacing the relevant files in the `broker-files` GCS bucket, which has been set up to stage the files used to run the nightly broker.
This can be done manually, or by re-running the setup using `setup_broker.sh`.
By design, the setup script(s) will skip the _creation_ of any resources that already exist, but it will still upload new broker files to the staging bucket, overwriting any existing files.

To __setup the production broker__ the process is exactly the same as for a testing instance, except that we set the testid to `False`.
This builds and uses GCP resources that do _not_ have a testid appended to the names.

The "teardown" options that can be triggered in the setup scripts are specifically configured to _prevent_ a user from deleting _production_ resources, even if the user intentionally tries to do so.
This is one layer of security, but we should still look into protecting those resources in other ways (e.g., make backups of databases and buckets, find out which resources can be configured directly to prevent deletion and configure them).

<!-- fe Create and deploy a production instance of the broker -->


# Detailed Workflow with code examples
<!-- fs -->
To test the broker, you should perform these 3 steps:
1. [__Setup a testing instance__](#1-setup-a-testing-instance). Setup your own instance of the broker for testing.
2. [__Run your tests__](#2-run-your-tests) using the instance you created in step 1.
3. [__Option a: Leave the testing instance inactive__](#3a-leave-the-testing-instance-inactive). Use this option if you are done for the day, but you want to come back to the same broker testing instance later. These are instructions to _stop/shutdown_ your GCP testing resources so that we do not pay for them to sit idle.
3. [__Option b: Teardown the testing instance__](#3b-teardown-the-testing-instance) when you are completely done with it (i.e., delete the GCP resources created in step 1).

__Note: All resources that are hyperlinked in what follows are _production_ resources.
The testing resources you create will have the same names with your chosen testid appended (and joined by either "-" or "\_").__

## 1. Setup a testing instance
<!-- fs -->
Follow the setup instructions in [../README.md](../README.md#setup-the-broker-for-the-first-time).
Most of step 1 should be done already, however, make sure your `GOOGLE_APPLICATION_CREDENTIALS` environment variable points to valid GCP credentials.
In step 2 you will install the needed tools/libraries.
Activate your virtual environment if needed.

```bash
#--- Set the project ID env variable if it's not already
export GOOGLE_CLOUD_PROJECT=ardent-cycling-243415

#--- The Compute Engine VMs must be assigned to a specific zone.
# We currently use the same zone for all instances.
# The default is us-central1-a,
# but it can controlled explicitly by setting an environment variable.
# This variable has not been fully implemented in other components of the
# broker (the zone is hardcoded in some places) yet.
# Therefore I do not recommend changing this yet.
# export CE_ZONE=us-central1-a

#--- Get the current broker repo/branch and navigate to the setup directory
git clone https://github.com/mwvgroup/Pitt-Google-Broker
cd Pitt-Google-Broker
git checkout u/tjr/broker-v0.2
cd broker/setup_broker

#--- Setup a testing instance of the broker, tagged with "mytest"
testid="mytest"
./setup_broker.sh $testid
# this script is described in the text below
```

Some notes:
- It may ask you to authenticate yourself using `gcloud auth login`;
follow the instructions.
- __Make sure the VM install scripts have completed__ before running the broker or stopping the instances. This can take __15 minutes__ or longer. Navigate to the instance in the GCP Console (see [View and Access Resources](#view-and-access-resources) below) and click the "Monitoring" tab. The CPU utilization will fall to >1% when it is done.
- You must __stop the VM instances *before* running the broker__ (see [Leave the testing instance inactive](#leave-the-testing-instance-inactive) below). The night conductor will start them up again.

See [What does `setup_broker.sh` do?](#what-does-setup_brokersh-do) for details.

The consumer VM (`ztf-consumer-mytest` in our example) requires two __authorization files__ to connect to the ZTF stream.
_These must be obtained independently and uploaded to the VM manually, stored at the following locations:_
    1. `krb5.conf`, at VM path `/etc/krb5.conf`
    2. `pitt-reader.user.keytab`, at VM path `/home/broker/consumer/pitt-reader.user.keytab`

You can use the `gcloud compute scp` command for this:
```bash
gcloud compute scp krb5.conf "ztf-consumer-${testid}:/etc/krb5.conf" --zone="$CE_ZONE"
gcloud compute scp pitt-reader.user.keytab "ztf-consumer-${testid}:/home/broker/consumer/pitt-reader.user.keytab" --zone="$CE_ZONE"
```

<!-- fe Setup a testing instance -->

## 2. Run your tests
<!-- fs -->
You can alter your testing instance of the broker at-will, and use it to execute any type of testing that you'd like.

In the example below, we use it to walk through the process of actually running the broker to ingest/store/process the ZTF alert stream.

If you would just rather __start an *individual* component of the broker__, here are some options:
- Look in [night_conductor/start_night/start_night.sh](night_conductor/start_night/start_night.sh) to see how `night-conductor` starts that component and mimic it. In most cases, you can simply call a shell script and pass in a few variables.
- Start a Dataflow job manually by following [beam/README.md](beam/README.md).
- Start the VMs manually:
```bash
# start both VMs setup by setup_broker.sh
testid=mytest
consumerVM="ztf-consumer-${testid}"
nconductVM="night-conductor-${testid}"
zone=us-central1-a
gcloud compute instances start "$consumerVM" "$nconductVM" --zone="$zone"
# we have not set any metadata attributes (see below for an explanation),
# so the VMs won't do anything except startup.
```

### [Example] Run the broker
<!-- fs -->
This example will walk you through running the broker to ingest, store, and process an alert stream.
__Before you begin, make sure your VMs are stopped__.
(If you have just set up your testing instance, your VMs are running.)
You can do this from the Console (see [View and Access Resources](#view-and-access-resources)) or the command line (see [Leave the testing instance inactive](#3a-leave-the-testing-instance-inactive)).

You have three options for _ingesting alerts_ into the broker:
1. Connect to ZTF's __live stream__. Obviously, this can only be done at night (Pacific Time) when there is a live stream to connect to.
2. Connect to a ZTF __stream from a previous night__. This will __*flood*__ the broker with alerts! See below for a heads up on what to expect. You can connect to any ZTF topic _within the last 7 days_ that _contains at least 1 alert_.
3. Use the __consumer simulator__ to __*control the flow*__ of alerts into the broker. This will bypass the broker's consumer component.

__I recommend option 3, unless you specifically want to test the consumer.__ The workflow will split below, depending on which ingestion option you want.

__Night Conductor__

The broker employs the [`night-conductor`](https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/night-conductor?tab=details&project=ardent-cycling-243415) VM to orchestrate starting and ending the night.
This VM runs the set of scripts in the [night_conductor/](night_conductor/) directory.
Its _startup script_, [night_conductor/vm_startup.sh](night_conductor/vm_startup.sh) (which calls the other scripts), contains the required logic to start and stop broker components/GCP resources.
(Note that this set of scripts is staged in the bucket [ardent-cycling-243415-broker_files](https://console.cloud.google.com/storage/browser/ardent-cycling-243415-broker_files?project=ardent-cycling-243415&pageState=%28%22StorageObjectListTable%22:%28%22f%22:%22%255B%255D%22%29%29&prefix=&forceOnObjectsSortingFiltering=false) along with files used by other components to run the broker.)

To control the behavior of the startup script, we pass arguments to it by setting _metadata attributes_ on `night-conductor` prior to starting the VM.
Metadata attributes are cleared before the VM shuts down so that no unexpected behavior occurs on the next startup.

- To run the broker end-to-end, proceed with the following section, [End-to-End](#end-to-end) .
- To bypass the broker's consumer and use the consumer simulator, skip to [Use the consumer simulator](#use-the-consumer-simulator)

#### __End-to-End__
<!-- fs -->
Here we connect to a ZTF alert stream and run the broker, end-to-end.
We do this using the broker's "night conductor" (see above for a description).

To start the broker for the night, we must set a metadata attribute with the name of the _ZTF/Kafka topic_ the consumer should subscribe to.
ZTF publishes alerts to a _new_ topic each night.
The syntax for the topic name is `ztf_yyyymmdd_programid1`.
You can connect to a live stream (if one is currently available) or to a topic from a previous night within the last 7 days.
__Before you connect to a previous night, please see the note below about _flooding_ the broker with alerts,__ and consider using the consumer _simulator_ instead (see next section).

Note: Our consumer's connection to the Kafka stream will fail _if there is not at least 1 alert in the topic_. The VM running the consumer will hang while trying to connect, and will require a _reboot_.
For topics from previous nights, you can check [ztf.uw.edu/alerts/public/](https://ztf.uw.edu/alerts/public/).
`tar` files larger than 74 (presumably in bytes) indicate dates with >0 alerts.

```bash
testid=mytest
instancename="night-conductor-${testid}"
zone=us-central1-a
broker_bucket="${GOOGLE_CLOUD_PROJECT}-broker_files-${testid}"

#--- Start the night
# set the attributes
NIGHT=START
KAFKA_TOPIC=ztf_20210120_programid1 # ztf_yyyymmdd_programid1
# must be within the last 7 days and contain at least 1 alert
gcloud compute instances add-metadata "$instancename" --zone="$zone" \
        --metadata NIGHT="$NIGHT",KAFKA_TOPIC="$KAFKA_TOPIC"
# the night conductor VM will get the testid by parsing its own instance name

# the startup script should already be set, but we can make sure
startupscript="gs://${broker_bucket}/night_conductor/vm_startup.sh"
gcloud compute instances add-metadata "$instancename" --zone "$zone" \
        --metadata startup-script-url="$startupscript"

# start the VM to trigger the startup script
gcloud compute instances start "$instancename" --zone "$zone"
# the VM will shut itself down automatically when broker startup is complete

#--- The broker will start ingesting the ZTF stream
#    and storing/processing the data.

#--- End the night
# set the attributes
NIGHT=END
gcloud compute instances add-metadata "$instancename" --zone="$zone" \
      --metadata NIGHT="$NIGHT"

# start the VM to trigger the startup script that will shutdown the broker
gcloud compute instances start "$instancename" --zone "$zone"
# the VM will shut itself down automatically when broker shutdown is complete
```

__Start Night Details:__

Currently, `night-conductor` executes the following to start the night:
1. Clears the messages from Pub/Sub subscriptions that we use to count the number of elements received and processed each night. Your subscription instances will have names like `<topic_name>-counter-<testid>`.
2. Deploys the two Dataflow jobs and waits for their status to change to "Running".
3. Starts the `ztf-consumer-<testid>` VM, which is configured with a startup script to connect to ZTF and begin ingesting.

Cloud Functions are always "on"; `night-conductor` does not manage them.

__Note on connecting to a previous night's topic (i.e., _flooding_ the broker with alerts):__

Based on my experience, _what to expect if you allow the broker to ingest a ZTF topic that already contains 200,000 - 400,000 alerts (a typical, active night) as fast as it can_:
- The consumer should ingest and publish all of the alerts to the `ztf_alerts` Pub/Sub topic within about 30 minutes.
- The Avro->GCS storage component (Cloud Function) should process all of the alerts within about 60 minutes.
- The BigQuery storage component (Dataflow job) will likely get bogged down and take >10 hours to process all of the alerts. I recommend just cancelling the job. This component performs reasonably at the live-stream rate, but it tends to struggle when flooded with alerts. In general, it is the component with the largest lag times and number of dropped alerts. It could use some TLC, or a complete reconfiguring.
- The value-added processing component (Dataflow job) should process all of the alerts within a few hours.

__Note on _triggering_ `night-conductor` to start the night:__

The consumer's connection to ZTF will fail if there is not as least 1 alert in the topic, but the terminal/shell is not released,
so we can't simply keep trying until the connection succeeds.
Therefore I am still manually triggering `night-conductor` to start the night after ZTF issues its first alert.
There _should_ be a programatic way to check whether a topic is available, but I haven't been able to find it yet.
I have a new lead, so I'll work on it more.


__End Night Details:__

Currently, `night-conductor` executes the following to end the night:
1. Stop the [`ztf-consumer`](https://console.cloud.google.com/compute/instancesMonitoringDetail/zones/us-central1-a/instances/ztf-consumer?project=ardent-cycling-243415&tab=monitoring&duration=PT1H&pageState=%28%22duration%22:%28%22groupValue%22:%22P7D%22,%22customValue%22:null%29%29) VM (which stops the ingestion).
2. Drains the Dataflow jobs (it stops accepting new alerts, finishes the processing of all alerts in the pipeline, and the job ends).

Note on triggering `night-conductor` to end the night:
There is no programatic way to check whether ZTF has sent its last alert for the night,
or to check whether we have received all the alerts ZTF has sent.
Christopher Phillips at ZTF says we can assume that ZTF has sent out all of its alerts by shortly after sunrise ZTF time.
I am still manually triggering `night-conductor` to end the night.
Over the last ~2 months (today is 1/19/21), ZTF has consistently been done issuing alerts by 9:30am ET, and our broker does not have a significant lag.
I plan to automate triggering `night-conductor` to end the night at 10am ET, but this will need to be adjusted seasonally.

<!-- fe End-to-End -->

#### Use the consumer simulator
<!-- fs -->
By using our "consumer simulator", you can feed alerts into your broker without connecting to a ZTF stream. This option bypasses the broker's consumer and gives you a lot more control over the ingestion. It is useful if:
- there is not a ZTF stream available that suites your needs; and/or
- you want to __*control the flow of alerts*__ into the system.

The ingestion rate of the _broker's_ consumer is at the mercy of ZTF. A ZTF live stream is ingested at the same rate at which ZTF publishes the alerts. A ZTF stream from a previous night will be ingested as fast as possible. In the second case, alerts _flood_ into the system. Most (but not all) broker components handle this without much trouble, but in general, this is not a reasonable way to run our tests.

__How it works__:

The module's code is at [../dev_utils/consumer_sims/ztf_consumer_sim.py](../dev_utils/consumer_sims/ztf_consumer_sim.py)

The consumer simulator publishes alerts to the `ztf_alerts-{testid}` Pub/Sub topic, from which all non-consumer components of the broker get the alerts.
The user can set the:
- `alertRate`: desired rate at which alerts are published
- `runTime`: desired length of time for which the simulator publishes alerts
- `publish_batch_every`: interval of time the simulator sleeps between publishing batches of alerts

The simulator publishs alerts in batches, so the desired alert rate and run time both get converted.
Rounding occurs so that an integer number of batches are published, each containing the same integer number of alerts.
Therefore the _alert publish rate and the length of time for which the simulator runs may not be exactly equal to the `alertRate` and `runTime`_ respectively.
If you want one or both to be exact, choose an appropriate combination of variables.

_[`ztf_alerts-reservoir`](https://console.cloud.google.com/cloudpubsub/subscription/detail/ztf_alerts-reservoir?project=ardent-cycling-243415)_:

The simulator's _source_ of alerts is the Pub/Sub _subscription_ `ztf_alerts-reservoir` (attached to the _topic_ `ztf_alerts`).
All users of the consumer simulator access the _same_ reservoir by default(*).
__Please be courteous, and do not drain the reservoir__(**).
Alerts expire from the subscription after 7 days (max allowed by Pub/Sub), so if ZTF has not produced many alerts in the last week, the reservoir will be low.
On the bright side, the alerts coming from the simulator/reservoir will always be recent.
_You can check the number of alerts currently in the reservoir by viewing the subscription in the GCP Console_ (click the link above, look for "Unacked message count")

(*) An equivalent subscription reservoir is created for your testing instance, but it is not pre-filled with alerts.
However, once you _do_ have alerts in your testing instance, you can use a keyword argument to point the simulator's source subscription to your own reservoir.
This will create a closed loop wherein the same set of alerts will flow from your reservoir, into your `ztf_alerts-{testid}` topic, and back into your reservoir (which is a subscription on that topic).
In this way, you can access an __infinite source of (non-unique) alerts__.
(You can also publish alerts to an arbitary topic via a keyword.)

(**) To facilitate this, in addition to using your own reservoir, you have the option to "nack" messages, which tells the subscriber _not_ to acknowledge the messages. As a result, the alerts will not disappear from the reservoir; the subscriber will redeliver the messages at an arbitrary time in the future (to you or someone else).

__Workflow__:
1. [Start the broker](#start-the-broker) using `night-conductor` with the metadata attribute that holds the ZTF/Kafka topic set to `NONE`. This instructs `night-conductor` to _skip_ booting up the consumer VM.
2. [Run the consumer simulator](#run-the-consumer-simulator):
Use the `ztf_consumer_sim` python module to feed alerts into the `ztf_alerts-{testid}` Pub/Sub topic that all _non-consumer_ components use to ingest alerts. The code for the module is nested under the `dev_utils` package at the top level of the repo: [../dev_utils/consumer_sims/ztf_consumer_sim.py](../dev_utils/consumer_sims/ztf_consumer_sim.py).
3. [Shutdown the broker](#shutdown-the-broker) ("end the night") using `night-conductor`. (This stops the broker components so that they are inactive and we do not continue paying for them; it does not delete your broker instance.)

Code follows.

##### Start the broker

```bash
testid=mytest
instancename="night-conductor-${testid}"
zone=us-central1-a
broker_bucket="${GOOGLE_CLOUD_PROJECT}-broker_files-${testid}"

#--- Start the broker without the consumer
# set the attributes
NIGHT="START"
KAFKA_TOPIC="NONE" # tell night-conductor to skip booting up consumer VM
gcloud compute instances add-metadata "$instancename" --zone="$zone" \
        --metadata NIGHT="$NIGHT",KAFKA_TOPIC="$KAFKA_TOPIC"
# night-conductor will get the testid by parsing its own instance name

# the startup script should already be set, but we can make sure
startupscript="gs://${broker_bucket}/night_conductor/vm_startup.sh"
gcloud compute instances add-metadata "$instancename" --zone "$zone" \
        --metadata startup-script-url="$startupscript"

# start the VM to trigger the startup script
gcloud compute instances start "$instancename" --zone "$zone"
# night-conductor shuts down automatically when broker startup is complete

#--- After a few minutes, the broker should be ready to process alerts.
#    Feed alerts into the system using the consumer simulator.

```

For more information, see:
- [View and Access Resources](#view-and-access-resources)
- [Where to look if there's a problem with `night-conductor`'s start/end night routines](#where-to-look-if-theres-a-problem-with-night-conductors-startend-night-routines)

##### Run the consumer simulator
Before starting this section, make sure your Dataflow jobs are running (if desired). Check the
[Dataflow jobs console](https://console.cloud.google.com/dataflow/jobs?project=ardent-cycling-243415).
When deployed, these jobs create _new_ subscriptions to the `ztf_alerts-{testid}` Pub/Sub topic, so _they will miss any alerts published to that topic prior to their deployment_.
(The Cloud Functions, by contrast, are always "on" and listening to the stream.)

You may need to update to the latest version (2.2.0) of the PubSub API: `pip install google-cloud-pubsub --upgrade`

```python
#--- Put the `dev_utils` directory on your path
# I will add this to the broker's setup instructions later.
import sys
# path_to_dev_utils = '/Users/troyraen/Documents/PGB/repo/dev_utils'
path_to_dev_utils = '/home/troy_raen_pitt/Pitt-Google-Broker/dev_utils'
sys.path.append(path_to_dev_utils)

#--- import the simulator
from consumer_sims import ztf_consumer_sim as zcs

#--- set some variables to use later
N = 10
aRate = 600

#--- Set desired alert rate. Examples:
alertRate = (aRate, 'perMin')  # (int, str)
    # unit (str) options: 'perSec', 'perMin', 'perHr', 'perNight'(=per 10 hrs)
alertRate = (N, 'once')
    # publish N alerts simultaneously, one time
alertRate = 'ztf-active-avg'  # = (300000, 'perNight')
    # avg rate for an avg, active night (range 150,000 - 450,000 perNight)
alertRate = 'ztf-live-max'  # = (200, 'perSec')
    # approx max incoming rate seen from live ZTF stream

#--- Set desired amount of time the simulator runs
runTime = (N, 'min')  # (int, str)
    # unit (str) options: 'sec', 'min', 'hr', 'night'(=10 hrs)
    # if alertRate "units" == 'once', setting runTime has no effect

#--- Set the rate at which batches are published (optional)
publish_batch_every = (5, 'sec')  # (int, str)
    # only str option is 'sec'
# In practice: the simulator publishes a batch, then sleeps for a time publish_batch_every.
# If you set a number that is too low, processing time will rival sleep time,
# and the actual publish rate and run time may be quite different than expected.
# I have only tested the default setting, which is (5, 'sec').

#--- Run the simulator (examples)
testid = 'mytest'

# publish N alerts, 1 time
alertRate = (N, 'once')
zcs.publish_stream(testid, alertRate)

# publish alerts for N minutes, at the average rate of an active ZTF night
alertRate = 'ztf-active-avg'
runTime = (N, 'min')
zcs.publish_stream(testid, alertRate, runTime)

# publish for N minutes, at avg rate of 30 alerts/sec, at a publish rate of 1 batch/min
alertRate = (30, 'perSec')
runTime = (N, 'min')
publish_batch_every = (60, 'sec')
zcs.publish_stream(testid, alertRate, runTime, publish_batch_every)

# Connect the simulator to your own reservoir,
# creating a closed loop between your data stream and reservoir.
# (Assumes you previously tapped the main reservoir and ingested at least 1 alert.)
sub_id = f'ztf_alerts-reservoir-{testid}'
zcs.publish_stream(testid, alertRate, runTime, sub_id=sub_id)

# Connect the simulator to a different sink (topic).
# By default, the simulator publishes to the topic `ztf_alerts-{testid}`,
# but you can publish to an arbitrary topic (which must already exist).
topic_id = f'troy_test_topic'
zcs.publish_stream(testid, alertRate, runTime, topic_id=topic_id)

# nack the messages so that they do not disappear from the reservoir
nack = True
zcs.publish_stream(testid, alertRate, runTime, nack=nack)
```

##### Shutdown the broker

Trigger `night-conductor` to end the night.
```bash
testid=mytest
instancename="night-conductor-${testid}"
zone=us-central1-a

#--- End the night
# set the attributes
NIGHT=END
gcloud compute instances add-metadata "$instancename" --zone="$zone" \
      --metadata NIGHT="$NIGHT"
# the night conductor VM will get the testid by parsing its own instance name

# start the VM to trigger the startup script that will shutdown the broker
gcloud compute instances start "$instancename" --zone "$zone"
# the VM shuts down automatically when broker shutdown is complete
```
<!-- fe Use the consumer simulator -->
<!-- fe Example: Run the broker -->
<!-- fe Run your tests -->

## 3a. Leave the testing instance inactive
<!-- fs -->
Use this option if you are done using your broker testing instance for the day, but you want to come back to the same instance later.
These are instructions to _stop/shutdown_ your GCP testing resources so that we do not pay for them to sit idle.
This is most important in the case of __Compute Engine VMs__ and __Dataflow jobs__ (which themselves utilize VMs).

__If you followed the "Run the broker" example above, all you need to do is _trigger the night conductor to end the night_.__ Instructions are in the example.

__Stop VMs manually:__

```bash
# stop both VMs setup by setup_broker.sh
testid=mytest
consumerVM="ztf-consumer-${testid}"
nconductVM="night-conductor-${testid}"
zone=us-central1-a
gcloud compute instances stop "$consumerVM" "$nconductVM" --zone="$zone"
```

Note: GCP also gives you the option to "suspend" (rather than "stop") the VM.
This is the equivalent of shutting the lid on your laptop; you can come back to it in the same state as when you left it. Charges _are_ incurred for VMs in the "suspend" state, but _not_ for VMs in the "stop" state.

__Stop Dataflow jobs manually:__

To stop/drain/cancel a Dataflow job from the commandline, you would need to look up the job ID assigned by Dataflow at runtime.
It's easier to stop the job manually from the [Dataflow console](https://console.cloud.google.com/dataflow/jobs?project=ardent-cycling-243415).
- Select your job, then select "Stop"
- You will be given the option to "Drain" or "Cancel"
    - Drain: stop ingesting alerts, but finish processing the alerts that are already in the pipeline.
    - Cancel: stop the job immediately. Ingestion stops and alerts already in the pipeline are dropped.

__Start things up again:__

Follow instructions in the section [Run your tests](#2-run-your-tests).

<!-- fe Leave the testing instance inactive -->

## 3b. Teardown the testing instance
<!-- fs -->
When you are completely done with your testing instance of the broker, __delete it__ by running the broker's setup script with the "teardown" argument set to `True`.

```bash
cd Pitt-Google-Broker/broker/setup_broker

# Teardown/delete all test resources with the testid "mytest"
testid="mytest"
teardown="True"
./setup_broker.sh $testid $teardown
```

This will delete all GCP resources tagged with the testid. You will be prompted several times to confirm.

<!-- fe Teardown the testing instance -->

<!-- fe Detailed Workflow with code examples -->


# Appendix

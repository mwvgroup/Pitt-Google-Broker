{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fa831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"Publish stream for ELAsTiCC using brokerClassification schema in the following repo.\n",
    "\n",
    "https://github.com/LSSTDESC/plasticc_alerts/blob/main/Examples/plasticc_schema\n",
    "\"\"\"\n",
    "\n",
    "import base64\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import fastavro\n",
    "from flask import Flask, request\n",
    "from google.cloud import logging\n",
    "\n",
    "from broker_utils import data_utils, gcp_utils, schema_maps\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "PROJECT_ID = os.getenv(\"GCP_PROJECT\")\n",
    "TESTID = os.getenv(\"TESTID\")\n",
    "SURVEY = os.getenv(\"SURVEY\")  # elasticc\n",
    "\n",
    "# connect to the logger\n",
    "logging_client = logging.Client()\n",
    "log_name = \"elasticc-stream\"  # same log for all broker instances\n",
    "logger = logging_client.logger(log_name)\n",
    "\n",
    "# GCP resources used in this module\n",
    "ps_topic = f\"{SURVEY}-classifications\"\n",
    "if TESTID != \"False\":  # attach the testid to the names\n",
    "    ps_topic = f\"{ps_topic}-{TESTID}\"\n",
    "\n",
    "schema_map = schema_maps.load_schema_map(SURVEY, TESTID)\n",
    "schema_out = fastavro.schema.load_schema(\"elasticc.v0_9.brokerClassfication.avsc\")\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=[\"POST\"])\n",
    "def index() -> Tuple[str, int]:\n",
    "    \"\"\"Entry point for Cloud Run trigger.\"\"\"\n",
    "    msg = _unpack_envelope(request.get_json())\n",
    "    # if there was an error, msg is a tuple. return immediately.\n",
    "    if isinstance(msg, Tuple):\n",
    "        return msg\n",
    "    # else, unpack\n",
    "    alert_dict, attrs = _unpack_message(msg)\n",
    "\n",
    "    # create the message for elasticc and publish the stream\n",
    "    avro = _create_elasticc_msg(alert_dict, attrs)\n",
    "    gcp_utils.publish_pubsub(ps_topic, avro)\n",
    "\n",
    "    return (\"\", 204)\n",
    "\n",
    "\"\"\"\n",
    "Description of what I think is happening for the function: index()\n",
    "- since this is the entry point for Cloud Run trigger, I'm assumming that the user will use the json file/key in\n",
    "  order to use the broker system. If a mistake is made, the system will ask the user to input the json file/key\n",
    "  once again.\n",
    "- if the json file/key is entered correctly, then the system will unpack what's contained in the file, and create\n",
    "  a message for elasticc and publish the stream using the information that is found within our existing repository\n",
    "  (specifically: gcp_utils)\n",
    "- I did some more reading on wikipedia on what avro is, and I see that the avro schema has the format of the \n",
    "  plasticc_schema we reviewed yesterday\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _unpack_envelope(envelope: dict) -> dict:\n",
    "    \"\"\"Check that the incoming envelope is valid and return the enclosed message.\"\"\"\n",
    "    if not envelope:\n",
    "        msg = (\"Bad Request: no Pub/Sub message received\", 400)\n",
    "        logger.log_text(err_msg, severity=\"DEBUG\")\n",
    "\n",
    "    elif isinstance(envelope, dict) or \"message\" not in envelope:\n",
    "        msg = (\"Bad Request: invalid Pub/Sub message format\", 400)\n",
    "        logger.log_text(err_msg, severity=\"DEBUG\")\n",
    "\n",
    "    else:\n",
    "        msg = envelope[\"message\"]\n",
    "\n",
    "    return msg\n",
    "\n",
    "\n",
    "def _unpack_message(msg: dict) -> Tuple[dict, dict]:\n",
    "    \"\"\"Unpack the alert.\"\"\"\n",
    "    alert_dict = data_utils.decode_alert(\n",
    "        base64.b64decode(msg[\"data\"]), drop_cutouts=True, schema_map=schema_map\n",
    "    )\n",
    "    return alert_dict, msg[\"attributes\"]\n",
    "\"\"\"\n",
    "Description of what I think is happening for the function: _unpack_message()\n",
    "- the variable alert_dict calls on our repository (data_utils) and unpacks/decodes msg[\"data\"]\n",
    "- from the function index(), we see that the variable msg calls on the function _unpack_envelope() to open and\n",
    "  review the information found within the json file/key. We also see that the schema_map is called on\n",
    "- finally, this function returns the attributes found within the variable msg as well as alert_dict\n",
    "\"\"\"\n",
    "\n",
    "def _create_elasticc_msg(alert_dict, attrs):\n",
    "    \"\"\"Create a message according to the ELAsTiCC broker classifications schema.\n",
    "\n",
    "    https://github.com/LSSTDESC/plasticc_alerts/blob/main/Examples/plasticc_schema\n",
    "    \"\"\"\n",
    "    # original elasticc alert as a dict\n",
    "    elasticc_alert, = alert_dict[\"alert\"]\n",
    "\n",
    "    # dict with the following keys:\n",
    "    #   schema_map[\"objectId\"]\n",
    "    #   schema_map[\"sourceId\"]\n",
    "    #   \"prob_class0\"\n",
    "    #   \"prob_class1\"\n",
    "    #   \"predicted_class\"\n",
    "    supernnova_results = alert_dict[\"SuperNNova\"]\n",
    "\n",
    "    # here are a few things you'll need\n",
    "    elasticcPublishTimestamp = attrs[\"kafka.timestamp\"]\n",
    "    brokerIngestTimestamp = attrs[\"ingest_time\"]  # Troy: attach this in first module\n",
    "    brokerVersion = \"v0.6\"\n",
    "\n",
    "    classifications = [\n",
    "        {\n",
    "            \"classifierName\": \"SuperNNova_v1.3\",  # Troy: pin version in classify_snn\n",
    "            # Chris: fill these two in. classIds are listed here:\n",
    "            #        https://docs.google.com/presentation/d/1FwOdELG-XgdNtySeIjF62bDRVU5EsCToi2Svo_kXA50/edit#slide=id.ge52201f94a_0_12\n",
    "            \"classifierParams\": \"\",  # leave this blank for now\n",
    "            \"classId\": ,\n",
    "            \"probability\": ,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Chris: fill this in with the correct key/value pairs.\n",
    "    #        it is the dictionary that will be sent to elasticc, so it needs these:\n",
    "    #        https://github.com/LSSTDESC/plasticc_alerts/blob/main/Examples/plasticc_schema/elasticc.v0_9.brokerClassification.avsc\n",
    "    msg = {}\n",
    "\n",
    "    # avro serialize the dictionary\n",
    "    avro = _dict_to_avro(msg, schema_out)\n",
    "\n",
    "    return avro\n",
    "\"\"\"\n",
    "- I think I understand what I need to do, I just have a few questions regarding how I should format the \n",
    "  classIds. I.e., there are sub classifications in the link you sent me. Are the classifications just\n",
    "  separated by commas (and also how should the subfields be formatted?)\n",
    "- similar question to the key/value pairs. I understand what needs to be inserted, I just have questions on how\n",
    "  the information should be formatted\n",
    "\"\"\"\n",
    "\n",
    "def _dict_to_avro(msg: dict, schema: dict):\n",
    "    \"\"\"Avro serialize a dictionary.\"\"\"\n",
    "    fout = io.BytesIO()\n",
    "    fastavro.writer(fout, schema, [msg])\n",
    "    fout.seek(0)\n",
    "    avro = fout.getvalue()\n",
    "    return avro\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PORT = int(os.getenv(\"PORT\")) if os.getenv(\"PORT\") else 8080\n",
    "\n",
    "    # This is used when running locally. Gunicorn is used to run the\n",
    "    # application on Cloud Run. See entrypoint in Dockerfile.\n",
    "    app.run(host=\"127.0.0.1\", port=PORT, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pgb]",
   "language": "python",
   "name": "conda-env-pgb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

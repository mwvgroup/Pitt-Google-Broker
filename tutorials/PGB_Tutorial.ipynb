{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PGB-Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwvgroup/Pitt-Google-Broker/blob/u%2Ftjr%2Ftutorials/tutorials/PGB_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnq45aCYshxk"
      },
      "source": [
        "# 1) Data Products\n",
        "\n",
        "Architecture figure highlighting the data products available to them\n",
        "\n",
        "Links to webpages:\n",
        "- [BQ table descriptions and schemas]\n",
        "- [Cloud Storage buckets]\n",
        "- [Pub/Sub streams]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NMjb0oOtxG7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_FJ8LH4omNX"
      },
      "source": [
        "# 2) Setup\n",
        "\n",
        "- Create GCP Project\n",
        "- Enable billing\n",
        "- Enable APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm8S4HbIQlK1"
      },
      "source": [
        "#--- Colab Setup\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()  # follow the instructions to authorize Google Cloud SDK \n",
        "\n",
        "# connect your file system (local or Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "# example of down/uploading a file (also see file browser on left)\n",
        "\n",
        "\n",
        "#--- Cloud Shell Setup\n",
        "\n",
        "\n",
        "#--- Local machine setup\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaFC3geYYfyn"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubkZqvJuuMrR"
      },
      "source": [
        "my_project_id = 'ardent-cycling-243415'\n",
        "pgb_project_id = 'ardent-cycling-243415'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feaPBbMmtaVu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FrTV1deWZFN"
      },
      "source": [
        "# 3) Query a BigQuery Database\n",
        "\n",
        "- [Query pricing](https://cloud.google.com/bigquery/docs/query-overview#query_pricing) (charges based on number of bytes processed; first 1 TB of data processed per month, per billing account, is __free__.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUjPP0ENkYlc"
      },
      "source": [
        "## 3a) Python\n",
        "\n",
        "standard SQL default ([change to legacy SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/enabling-standard-sql#changing_from_the_default_dialect))\n",
        "\n",
        "Links with more info:\n",
        "- [Documentation](https://googleapis.dev/python/bigquery/latest/index.html)\n",
        "- [Colab Snippets](https://colab.research.google.com/notebooks/snippets/bigquery.ipynb#scrollTo=jl97S3vfNHdz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhhHT9YArfyd"
      },
      "source": [
        "# create a connection to BigQuery\n",
        "bq_client = bigquery.Client(project=my_project_id)\n",
        "\n",
        "# write the SQL query as a string\n",
        "sql = \"\"\"\n",
        "    SELECT name, SUM(number) as count\n",
        "    FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
        "    GROUP BY name\n",
        "    ORDER BY count DESC\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "# make an API request\n",
        "query_job = bq_client.query(sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUvYkm0cQS0a"
      },
      "source": [
        "# Option1: dump results to a pandas.DataFrame\n",
        "df = query_job.to_dataframe()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaLecK6YYWJI"
      },
      "source": [
        "# Option 2: parse results row by row\n",
        "print(\"The query data:\")\n",
        "for row in query_job:\n",
        "    # row values can be accessed by field name or index\n",
        "    print(f\"name={row[0]}, count={row['count']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nDSzXh1khgt"
      },
      "source": [
        "## 3b) Command-line tool `bq`\n",
        "\n",
        "legacy SQL default ([change to standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/enabling-standard-sql#changing_from_the_default_dialect))\n",
        "\n",
        "Links with more info:\n",
        "- [Running queries from the bq command-line tool](https://cloud.google.com/bigquery/docs/bq-command-line-tool#running_queries_from_the) (also see other sections on this page)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q4hxE8QVrL3"
      },
      "source": [
        "\n",
        "\n",
        "```bash\n",
        "#--- retrieve query results from command line\n",
        "# (this cell is not executable)\n",
        "\n",
        "bq query --use_legacy_sql=false \\\n",
        "'SELECT\n",
        "  word,\n",
        "  SUM(word_count) AS count\n",
        "FROM\n",
        "  `bigquery-public-data`.samples.shakespeare\n",
        "WHERE\n",
        "  word LIKE \"%raisin%\"\n",
        "GROUP BY\n",
        "  word'\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qu69YBOtfkM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxcGxp_1WkK8"
      },
      "source": [
        "# 4) Download a file from Cloud Storage\n",
        "\n",
        "- grab cutouts from Cloud Storage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtBrGafqO7gr"
      },
      "source": [
        "#--- Download an alert from GCS bucket\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "bucket_name = f'{pgb_project_id}_ztf_alert_avro_bucket'\n",
        "fname_prefix = '1605062619785'  # recently ingested avro file name, get from logging\n",
        "local_dir = './testdownload'\n",
        "delimiter = '/'\n",
        "\n",
        "storage_client = storage.Client.from_service_account_json('GCPauth_pitt-google-broker-prototype-0679b75dded0.json')\n",
        "bucket = storage_client.get_bucket(bucket_name)\n",
        "blobs = bucket.list_blobs(prefix=fname_prefix, delimiter=delimiter) #List all objects that satisfy the filter.\n",
        "# Download the file to a destination\n",
        "# Iterating through for loop one by one using API call\n",
        "for blob in blobs:\n",
        "    destination_uri = '{}/{}'.format(local_dir, blob.name)\n",
        "    blob.download_to_filename(destination_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlnP_NoGvnQt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxdGsenapMZR"
      },
      "source": [
        "# 5) Listen to Pub/Sub Streams\n",
        "\n",
        "- Python\n",
        "  - unpack into dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEXE5Ek8vkkX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luxJJpZom4rJ"
      },
      "source": [
        "# 6) BigQuery ML\n",
        "\n",
        "- [What is BigQuery ML?](https://cloud.google.com/bigquery-ml/docs/introduction)\n",
        "- [Quickstart using the Cloud Console](https://cloud.google.com/bigquery-ml/docs/bigqueryml-web-ui-start)\n",
        "- [Applying Machine Learning to your Data with GCP](https://www.coursera.org/learn/data-insights-gcp-apply-ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh5GTskUm-lH"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEMBJBYKsiVW"
      },
      "source": [
        "# 7) Filter or Process the Data (using Apache Beam)\n",
        "\n",
        "We use the [Apache Beam](https://beam.apache.org/) (Python) SDK to write the data pipeline (see also [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/)). This has the following advantages:\n",
        "\n",
        "1. Convenience: Native I/O functions for Pub/Sub, BigQuery, and Cloud Storage.\n",
        "2. Flexiblity: The same pipeline can accept streaming and batch inputs. -> Use the same pipeline to process the live stream and reprocess the database.\n",
        "3. Portability: The same pipeline can be run/executed in multiple environments (Google Cloud, AWS, local machine) via an execution \"runner\" ([Apache Flink](https://beam.apache.org/documentation/runners/flink/), [Apache Spark](https://beam.apache.org/documentation/runners/spark/), [Google Dataflow](https://beam.apache.org/documentation/runners/dataflow/), or [direct/local](https://beam.apache.org/documentation/runners/direct/)).\n",
        "\n",
        "Links with more info:\n",
        "- [Colab Snippets: Apache Beam](https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-py.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecuUx8mGwbR-"
      },
      "source": [
        "#--- Install Apache Beam (and other setup)\n",
        "\n",
        "# Run and print a shell command.\n",
        "def run(cmd):\n",
        "  print('>> {}'.format(cmd))\n",
        "  !{cmd}\n",
        "  print('')\n",
        "\n",
        "# Install apache-beam.\n",
        "run('pip install --quiet apache-beam')\n",
        "\n",
        "# Copy the input file into the local file system.\n",
        "run('mkdir -p data')\n",
        "run('gsutil cp gs://dataflow-samples/shakespeare/kinglear.txt data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVz_RyMSwUzK"
      },
      "source": [
        "#--- Minimal word count (example)\n",
        "\n",
        "import apache_beam as beam\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/*'\n",
        "outputs_prefix = 'outputs/part'\n",
        "\n",
        "# Running locally in the DirectRunner.\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (\n",
        "      pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
        "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "      | 'Format results' >> beam.Map(lambda word_count: str(word_count))\n",
        "      | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
        "  )\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "run('head -n 20 {}-00000-of-*'.format(outputs_prefix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2RAwDK1Pz-n"
      },
      "source": [
        "\n",
        "import apache_beam as beam\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn8-S4sxNltt"
      },
      "source": [
        "# Functions and classes that will be packaged into a module \n",
        "# and hidden from the user.\n",
        "\n",
        "class extractAlertDict(beam.DoFn):\n",
        "    def process(self, msg):\n",
        "        from io import BytesIO\n",
        "        from fastavro import reader\n",
        "\n",
        "        # Extract the alert data from msg -> [dict]\n",
        "        with BytesIO(msg) as fin:\n",
        "            alertDicts = [r for r in reader(fin)]  # contains 1 dict\n",
        "\n",
        "        return alertDicts\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hng_ZsO4VDk3"
      },
      "source": [
        "# Functions and classes for the user to experiment with.\n",
        "\n",
        "#--- Filters\n",
        "# https://beam.apache.org/documentation/transforms/python/elementwise/filter/\n",
        "\n",
        "# example filter for likely solar system objects\n",
        "def nearby_ssobject(alert):\n",
        "    candidate = alert['candidate']\n",
        "    ssobject_is_near = (candidate['ssdistnr'] > 0) and (candidate['ssdistnr'] < 5)\n",
        "    return ssobject_is_near\n",
        "\n",
        "# write your own filter\n",
        "def my_filter(alert):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        alert (dict): a single alert\n",
        "    Returns:\n",
        "        keep_alert (bool): True to keep the alert, False to filter it out\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "#--- Other Functions (arbitrary processing)\n",
        "# \n",
        "\n",
        "class my_function(beam.DoFn):\n",
        "    def process(self, alertDict):\n",
        "\n",
        "        # Process the alert however you want\n",
        "        # here we simply extract the object and candidate ids\n",
        "        oid, cid = alertDict['objectId'], alertDict['candid']\n",
        "        ids = {'objectId': oid, 'candid': cid}\n",
        "\n",
        "        # Return your output in a list\n",
        "        return [ids]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmS2_DjrF40D"
      },
      "source": [
        "# Run a Beam pipeline\n",
        "\n",
        "topic = 'ztf_alert_data-tjr'\n",
        "\n",
        "with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "    (\n",
        "        pipeline\n",
        "        # read Pub/Sub messages (bytes) into the pipeline\n",
        "        | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(topic=topic)\n",
        "        # extract the alert bytes into a dict\n",
        "        | 'ExtractAlertDict' >> beam.ParDo(extractAlertDict()))\n",
        "        # filter, keep alerts likely to be a solar system object\n",
        "        | 'nearby_ssobject' >> beam.Filter(nearby_ssobject)\n",
        "        # do some processing\n",
        "        | 'my_function' >> beam.ParDo(my_function())\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PGB-Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4ZEcM1Bnavm4",
        "cCcdjz1v3fuJ",
        "a-5iWHDq13Np",
        "W_JYHktO2Q7V",
        "Cw-BDfrmoCHN",
        "pdVlJmWWawvx",
        "tXTEuyccOJoa",
        "gVNNwPBfOBrZ",
        "s2kX-kEJemYD",
        "AbTyVXUko6-d",
        "FLEcUP3jtVh9",
        "ob8VDnewmsbg",
        "id1fk4u06Bcq",
        "3nDSzXh1khgt",
        "jwxBVDcgVsOZ",
        "bHoxCS-EAH4J",
        "WolBMdS2K1OV",
        "6Xz7JjM0_ow9",
        "z-TsZDqwDC8O",
        "VbkGy7YPV8IH",
        "0lM2UnLgY5bn",
        "bXGtYU4aGg5h",
        "XaZ4wfvY55bm",
        "O3yXrkBVlNKn",
        "u7F6Pa7M2RDv"
      ],
      "authorship_tag": "ABX9TyNGxhxurd0Hqa7+BR2jy4Ci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwvgroup/Pitt-Google-Broker/blob/u%2Ftjr%2Ftutorials/pgb_utils/tutorials/PGB_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLKPXqDBCCHh"
      },
      "source": [
        "# Pitt-Google Broker Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_oOEo3Rr8cP"
      },
      "source": [
        "| | Learning Objective | Section |\n",
        "|---|---|---|\n",
        "| 1. | Understand What, Where, How to access our ZTF data | 1) Introduction |\n",
        "| 2. | Access our data | 3) BigQuery Database |\n",
        "| | | 4) Files in Cloud Storage |\n",
        "| 3. | Process our data | 5) Apache Beam data pipelines |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZEcM1Bnavm4"
      },
      "source": [
        "# Notebook Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZSn7nx19UVP"
      },
      "source": [
        "1. Introduction\n",
        "    - 1a. Data overview\n",
        "    - 1b. `pgb_utils` overview\n",
        "    - 1c. Note on costs\n",
        "\n",
        "2. Setup\n",
        "\n",
        "3. BigQuery Database\n",
        "    - 3a. Python\n",
        "        - Table names and schemas\n",
        "        - Query lightcurves and other history\n",
        "            - Plot a lightcurve\n",
        "        - Cone Search\n",
        "        - Direct access using `google.cloud.bigquery`\n",
        "    - 3b. Command-line tool `bq`\n",
        "\n",
        "4. Files in Cloud Storage\n",
        "    - 4a. Python\n",
        "        - Download files\n",
        "        - Plot cutouts and lightcurves\n",
        "    - 4b. Command-line tool `gsutil`\n",
        "\n",
        "5. Apache Beam data pipelines\n",
        "    - 5a. A demo example\n",
        "    - 5b. Descriptions\n",
        "    - 5c. Pitt-Google working examples\n",
        "        - Lightcurve pipeline\n",
        "        - Cone Search pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htm4fDjtonqP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCcdjz1v3fuJ"
      },
      "source": [
        "# 1) Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-5iWHDq13Np"
      },
      "source": [
        "## 1a) Data overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC2s1Xw61E5G"
      },
      "source": [
        "We are currently ingesting the [Zwicky Transient Facility](https://www.ztf.caltech.edu/) (ZTF) nightly alert stream and publishing various filtered and derived message streams. Our database and file storage contain data from alerts since ~November 2020.\n",
        "All Pitt-Google Broker data is public and hosted by [Google Cloud Platform](https://cloud.google.com/) (GCP).\n",
        "\n",
        "__Data Access__:\n",
        "\n",
        "All data and resources can be accessed via Google's [Cloud SDK](https://cloud.google.com/sdk) by way of the command-line, Python, and many other languages.\n",
        "In addition, we are developing the `pgb_utils` Python package which provides convience functions for common tasks such as querying the database for lightcurves or cone searches, decoding, plotting, and processing the data.\n",
        "This tutorial demonstrates access via the command-line (Cloud SDK) and Python (Cloud SDK, `pgb_utils`).\n",
        "\n",
        "In order to make API calls you will need to create a Cloud project that is associated with your Google account.\n",
        "The Setup section of the tutorial will walk you through this.\n",
        "You do not need to enable billing for this tutorial; everything we do will fall under the \"Free Tier\".\n",
        "Some notes on pricing are included in relevant sections below. \n",
        "\n",
        "__Data Products__:\n",
        "\n",
        "- Databases ([BigQuery](https://cloud.google.com/bigquery))\n",
        "    - query for:\n",
        "        - alerts (except cutouts)\n",
        "        - object lightcurves and other history\n",
        "        - cone searches\n",
        "\n",
        "- File storage ([Cloud Storage](https://cloud.google.com/storage))\n",
        "    - download the complete, original alert packets in Avro format, including cutouts\n",
        "\n",
        "- Message streams ([Pub/Sub](https://cloud.google.com/pubsub/docs/overview)) (covered in a future tutorial)\n",
        "    - streams include:\n",
        "        - ZTF stream: complete\n",
        "        - ZTF stream: filtered for purity\n",
        "        - ZTF stream: filtered for likely extragalactic transients\n",
        "        - ZTF stream + Salt2 fits (for likely extragalactic transients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NKaJK3K2I1I"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_JYHktO2Q7V"
      },
      "source": [
        "## 1b) `pgb_utils` overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMqmWgFn2NS-"
      },
      "source": [
        "`pgb_utils` is a collection of helper functions to facilitate interaction with Pitt-Google Broker data. The tutorial will demonstrate its use. The package is essentially a set of:\n",
        "\n",
        "1. Convience wrappers for the [Google Cloud Python SDK](https://cloud.google.com/python/docs/reference)\n",
        "2. Helper functions for decoding and plotting ZTF data, provided by ZTF (see [Filtering_alerts.ipynb](https://github.com/ZwickyTransientFacility/ztf-avro-alert/blob/master/notebooks/Filtering_alerts.ipynb))\n",
        "3. Helper functions for running [Apache Beam](https://beam.apache.org/) pipelines\n",
        "\n",
        "You are encouraged to look at and alter the source code to learn how to use the underlying methods yourself.\n",
        "\n",
        "Modules and their functionality include:\n",
        "\n",
        "- `pgb_utils.beam`\n",
        "    - helper functions for running Apache Beam data pipelines\n",
        "\n",
        "- `pgb_utils.bigquery`\n",
        "    - view dataset, table, and schema information\n",
        "    - query: lightcurves\n",
        "    - query: cone search\n",
        "    - cast query results to a `pandas.DataFrame` or `json` formatted string.\n",
        "\n",
        "- `pgb_utils.figures`\n",
        "    - plot lightcurves\n",
        "    - plot cutouts\n",
        "\n",
        "- `pgb_utils.utils`\n",
        "    - general utilities such as data type casting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0kapV453LsS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw-BDfrmoCHN"
      },
      "source": [
        "## 1c) Note on costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vE16R7x4yJB"
      },
      "source": [
        "Google Cloud's pricing structure is \"pay-as-you-go\". \n",
        "Most services include a [Free Tier](https://cloud.google.com/free) monthly allotment. \n",
        "For example, when querying BigQuery, the price is $5.00 per TB of data processed, and your first 1 TB per month is free. \n",
        "\n",
        "_You do not need to setup billing for this tutorial; everything we do will remain well within the Free Tier(*)._\n",
        "If you wish to become a power-user in the future, you may need to [create a billing account](https://support.google.com/cloud/answer/6293499#enable-billing). See also: \n",
        "- [pricing structure](https://cloud.google.com/pricing) (scroll to \"Only pay for what you use\")\n",
        "- [detailed price list](https://cloud.google.com/pricing/list) (search for \"BigQuery\", \"Cloud Storage\", \"Pub/Sub\"); \n",
        "- [pricing calculator](https://cloud.google.com/products/calculator?skip_cache=true) (same search as above)\n",
        "\n",
        "(*) The Free Tier for Cloud Storage is limited to US regions. We are interested in feedback from users outside the US on the behavior of that section of this tutorial for you. Please see the \"Files in Cloud Storage\" section of this tutorial for details. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALIGt4P12KB-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdVlJmWWawvx"
      },
      "source": [
        "# 2) Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qjOE_7Ka1zR"
      },
      "source": [
        "1. Use the [Cloud Resource Manager](https://console.cloud.google.com/cloud-resource-manager) to create a GCP project. Take note of the auto-generated \"Project ID\", you will need it below in order to make API calls.\n",
        "    - If you work outside of Colab you may need to:\n",
        "        - enable APIs for your project: [BigQuery](https://console.cloud.google.com/flows/enableapi?apiid=bigquery); \n",
        "        - [setup authentication](https://cloud.google.com/docs/authentication/getting-started)\n",
        "\n",
        "2. Installs, imports, etc.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXFXbjlVYFvZ"
      },
      "source": [
        "from google.colab import auth"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm8S4HbIQlK1"
      },
      "source": [
        "auth.authenticate_user()\n",
        "# follow the instructions to authorize Google Cloud SDK "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwISaJSymrhh"
      },
      "source": [
        "pgb_project_name = 'pitt-google-broker-prototype'\n",
        "pgb_project_id = 'ardent-cycling-243415'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubkZqvJuuMrR"
      },
      "source": [
        "# ENTER YOUR GCP PROJECT ID HERE\n",
        "\n",
        "my_project_id = "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKcureCHfJRu"
      },
      "source": [
        "# Create a function to run and print a shell command.\n",
        "def run(cmd: str):\n",
        "  print('>> {}'.format(cmd))\n",
        "  !{cmd}\n",
        "  print('')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2RAwDK1Pz-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc08186f-8a6b-4925-c002-0596320ca7d8"
      },
      "source": [
        "# Install some packages/versions not included with Colab\n",
        "# \n",
        "packages = ['apache-beam','google-apitools','APLpy',\n",
        "            'beautifulsoup4==4.8',  # plot_cutouts grayscale stretch='arcsinh'\n",
        "            'astropy-healpix==0.6',  # plot_cutouts grayscale stretch='arcsinh'\n",
        "            'astropy==3.2.1',  # plot_cutouts grayscale stretch='arcsinh'\n",
        "            ]\n",
        "# --DEBUG\n",
        "# 'google-cloud-pubsub',] \n",
        "# causes:\n",
        "# `ContextualVersionConflict: (PyYAML 3.13 (/usr/local/lib/python3.7/dist-packages), Requirement.parse('pyyaml>=5.2'), {'libcst'})`\n",
        "# when importing apache-beam below.\n",
        "# --DEBUG\n",
        "\n",
        "for package in packages:\n",
        "    run(f'pip install --quiet {package}')\n",
        "# it may complain about some version conflicts but it should not cause problems.\n",
        "\n",
        "# Note: Outside of Colab you should use `pip install apache-beam[gcp]`\n",
        "# to get Beam's Google Cloud Platform tools."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> pip install --quiet apache-beam\n",
            "\u001b[K     |████████████████████████████████| 9.0MB 10.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 829kB 53.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 4.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 53.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 45.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.7MB 209kB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 63.3MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: multiprocess 0.70.11.1 has requirement dill>=0.3.3, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\n",
            ">> pip install --quiet google-apitools\n",
            "\u001b[K     |████████████████████████████████| 174kB 12.7MB/s \n",
            "\u001b[?25h  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            ">> pip install --quiet APLpy\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 378kB 19.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 31.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 42.4MB/s \n",
            "\u001b[?25h  Building wheel for pyregion (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            ">> pip install --quiet beautifulsoup4==4.8\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.6MB/s \n",
            "\u001b[?25h\n",
            ">> pip install --quiet astropy-healpix==0.6\n",
            "\n",
            ">> pip install --quiet astropy==3.2.1\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 9.4MB/s \n",
            "\u001b[?25h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL1HlPaGe5W5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bd45ce-e645-437e-d8a9-c4dded962fb3"
      },
      "source": [
        "# Install the Pitt-Google Broker utilities package\n",
        "run('python3 -m pip install --index-url https://test.pypi.org/simple/ --no-deps pgb_utils')\n",
        "\n",
        "# This is currently on the test.pypi servers. I should publish to the actual\n",
        "# PyPI soon, but I'd like some code review first."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> python3 -m pip install --index-url https://test.pypi.org/simple/ --no-deps pgb_utils\n",
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Collecting pgb_utils\n",
            "  Downloading https://test-files.pythonhosted.org/packages/bd/37/544e9d6efb27cc6349c7189109bfd821643709a4de30187c6bd12db682b8/pgb_utils-0.0.11-py3-none-any.whl\n",
            "Installing collected packages: pgb-utils\n",
            "Successfully installed pgb-utils-0.0.11\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaFC3geYYfyn"
      },
      "source": [
        "import apache_beam as beam\n",
        "import aplpy\n",
        "from astropy import coordinates as coord\n",
        "from astropy import units as u\n",
        "from astropy.io import fits\n",
        "import fastavro\n",
        "from google.colab import drive\n",
        "import gzip\n",
        "import io\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "# import warnings\n",
        "\n",
        "from google.cloud import bigquery, storage\n",
        "# , pubsub\n",
        "\n",
        "import pgb_utils as pgb"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqvwOxg40f-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab6bda5-6174-47cf-e1fe-57725eaa908a"
      },
      "source": [
        "# Connect your Google Drive file system\n",
        "# to be used in the sections on File Storage and Apache Beam pipelines\n",
        "drive.mount('/content/drive') \n",
        "# follow the instructions to authorize access\n",
        "\n",
        "# create a path for later\n",
        "colabpath = '/content/drive/MyDrive/Colab\\ Notebooks/PGB'\n",
        "colabpath_noesc = '/content/drive/MyDrive/Colab Notebooks/PGB'\n",
        "run(f'mkdir -p {colabpath}')\n",
        "\n",
        "# Colab Hint: Click the \"Files\" icon on the left to view a file browser.\n",
        "\n",
        "# Colab Hint: Click the \"Code snippets\" icon (<>) on the left and search for \n",
        "#       \"access drive\" to learn how to interact with Drive."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            ">> mkdir -p /content/drive/MyDrive/Colab\\ Notebooks/PGB\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P6I0SsNNEFk"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXTEuyccOJoa"
      },
      "source": [
        "# 3) BigQuery Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2JAzIQ9OBrY"
      },
      "source": [
        "Some links if you're interested. Otherwise, skip down to the code.\n",
        "\n",
        "- [Overview of querying BigQuery data](https://cloud.google.com/bigquery/docs/query-overview)\n",
        "\n",
        "- [Intro to BigQuery API Client Libraries](https://cloud.google.com/bigquery/docs/reference/libraries) (includes [Python](https://googleapis.dev/python/bigquery/latest/index.html), Java, C#, Go, Node.js, PHP, and Ruby)\n",
        "\n",
        "- [BigQuery How-to guides](https://cloud.google.com/bigquery/docs/how-to). There are many! Here are some of note:\n",
        "    - [Introduction to interacting with BigQuery](https://cloud.google.com/bigquery/docs/interacting-with-bigquery)\n",
        "\n",
        "    - [Queries](https://cloud.google.com/bigquery/docs/running-queries) (covered in this tutorial)\n",
        "\n",
        "    - [Writing query results to your own BigQuery table](https://cloud.google.com/bigquery/docs/writing-results#permanent-table) (also see other sections on that page)\n",
        "\n",
        "    - [Export table data to a file in Google Cloud Storage](https://cloud.google.com/bigquery/docs/exporting-data)\n",
        "        - Note the [Exporting data stored in BigQuery](https://cloud.google.com/bigquery/docs/exporting-data#exporting_data_stored_in) section of that page.\n",
        "\n",
        "    - [Best practices](https://cloud.google.com/bigquery/docs/how-to#best-practices) (e.g., Controlling costs, Optimizing query performance)\n",
        "\n",
        "- Other Colab tutorials:\n",
        "\n",
        "    - [Getting started with BigQuery](https://colab.research.google.com/notebooks/bigquery.ipynb)\n",
        "\n",
        "- Pricing:\n",
        "    - [Overview of BigQuery pricing](https://cloud.google.com/bigquery/pricing)\n",
        "        - [Query pricing](https://cloud.google.com/bigquery/docs/query-overview#query_pricing). Query charges are based on number of bytes processed. The first 1 TB of data processed per month, per billing account, is free. Beyond this, the price is $5.00 per TB.\n",
        "        - [Storage pricing](https://cloud.google.com/bigquery/pricing#storage). The first 10 GB of BigQuery storage per month is free (you only pay for tables you create in your own project).\n",
        "\n",
        "    - [Using cached query results](https://cloud.google.com/bigquery/docs/cached-results)\n",
        "        - BigQuery writes all query results to a temporary (~24 hours), cached results table (unless you explicitly specify a destination table). When you run a duplicate query, BigQuery attempts to reuse cached results. If it is successful, you are not charged for the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVNNwPBfOBrZ"
      },
      "source": [
        "## 3a) Python\n",
        "\n",
        "- [Python Client Documentation](https://googleapis.dev/python/bigquery/latest/index.html)\n",
        "- [Colab Snippets](https://colab.research.google.com/notebooks/snippets/bigquery.ipynb#scrollTo=jl97S3vfNHdz) (more examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2kX-kEJemYD"
      },
      "source": [
        "### Table names and schemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNGTZsy8xw_3",
        "outputId": "d8bf590f-3709-44c7-93aa-55406192cefc"
      },
      "source": [
        "# Create a Client for the BigQuery connections below\n",
        "pgb.bigquery.create_client(my_project_id)\n",
        "# this is just a convenience wrapper, as are many pgb functions. look at\n",
        "# its source code for guidance on using `google.cloud` libraries directly.\n",
        "\n",
        "# Colab Hint: Mouse over the function name to see its definition and\n",
        "#             source code (make sure the cell is selected)."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Instantiating a BigQuery client with project_id: ardent-cycling-243415\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sfxgU9AeuVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a73a00-485b-4194-d76b-226e1b59aaa6"
      },
      "source": [
        "pgb.bigquery.get_dataset_table_names()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['salt2', 'candidates', 'alerts']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNuQD35Zq7yJ"
      },
      "source": [
        "pgb.bigquery.get_table_info('candidates')\n",
        "\n",
        "# Colab Hint: Right-click this cell and select \"Copy to scratch cell\"\n",
        "#             so you can use this as a reference in later queries."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdy9BeSKT60F"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbTyVXUko6-d"
      },
      "source": [
        "### Query lightcurves and other history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7VY4RP6748V"
      },
      "source": [
        "# Choose the history data you want returned\n",
        "columns = ['jd', 'fid', 'magpsf', 'sigmapsf']\n",
        "# 'objectId' and 'candid' will be included automatically\n",
        "# options are in the 'candidates' table\n",
        "# pgb.bigquery.get_table_info('candidates')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxEnRTvie2DK"
      },
      "source": [
        "# Choose specific objects (optional. we'll choose some to minimize load time)\n",
        "objectIds = ['ZTF18aczuwfe', 'ZTF18aczvqcr', 'ZTF20acqgklx', 'ZTF18acexdlh']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4JfqzMTXIpk"
      },
      "source": [
        "To retrieve lightcurves and other history, we must query for the objects' \"candidate\" observations and aggregate the results by `objectId`.\n",
        "- `pgb.bigquery.query_objects()` is a convenience wrapper for this.\n",
        "It's options are demonstrated below. \n",
        "- You can also use `pgb.bigquery.object_history_sql_statement()` to get the required SQL statement and make the query yourself. See the \"Using `google.cloud.bigquery` directly\" section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtJ2C1gS9vdC"
      },
      "source": [
        "# Option 1: Get a single DataFrame of all results\n",
        "\n",
        "lcs_df = pgb.bigquery.query_objects(columns, objectIds=objectIds)\n",
        "# This will execute a dry run and tell you how much data will be processed.\n",
        "# You will be asked to confirm before proceeding.\n",
        "# In the future we'll skip this using\n",
        "dry_run = False\n",
        "\n",
        "lcs_df.sample(10)\n",
        "# cleaned of duplicates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrCb9wKBg-JC"
      },
      "source": [
        "Congratulations! You've now retrieved your first data from the transient table. \n",
        "It is a DataFrame containing the candidate observations for every object we requested, indexed by `objectId` and `candid` (candidate ID). It includes the columns we requested in the query.\n",
        "\n",
        "`fid` is the filter, mapped to an integer. You can see the filter's common name in the table schema we looked at earlier, or you can use `pgb.utils.ztf_fid_names()` which returns a dictionary of the mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "SS30ZgYzkPTY",
        "outputId": "2db28c8d-35f4-4586-cf95-b46b7d1c8140"
      },
      "source": [
        "fid_names = pgb.utils.ztf_fid_names()  # dict\n",
        "print(fid_names)\n",
        "\n",
        "lcs_df['filter'] = lcs_df['fid'].map(fid_names)\n",
        "lcs_df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'g', 2: 'R', 3: 'i'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>jd</th>\n",
              "      <th>fid</th>\n",
              "      <th>magpsf</th>\n",
              "      <th>sigmapsf</th>\n",
              "      <th>filter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>objectId</th>\n",
              "      <th>candid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">ZTF18aczuwfe</th>\n",
              "      <th>1440303993415015003</th>\n",
              "      <td>2.459195e+06</td>\n",
              "      <td>2</td>\n",
              "      <td>19.807590</td>\n",
              "      <td>0.159926</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1440322513415015004</th>\n",
              "      <td>2.459195e+06</td>\n",
              "      <td>1</td>\n",
              "      <td>19.819330</td>\n",
              "      <td>0.127621</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1442328323415015002</th>\n",
              "      <td>2.459197e+06</td>\n",
              "      <td>1</td>\n",
              "      <td>19.782700</td>\n",
              "      <td>0.172808</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1444285643415015001</th>\n",
              "      <td>2.459199e+06</td>\n",
              "      <td>2</td>\n",
              "      <td>19.927372</td>\n",
              "      <td>0.209740</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1449226053415015000</th>\n",
              "      <td>2.459204e+06</td>\n",
              "      <td>2</td>\n",
              "      <td>19.965925</td>\n",
              "      <td>0.170471</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            jd  fid     magpsf  sigmapsf filter\n",
              "objectId     candid                                                            \n",
              "ZTF18aczuwfe 1440303993415015003  2.459195e+06    2  19.807590  0.159926      R\n",
              "             1440322513415015004  2.459195e+06    1  19.819330  0.127621      g\n",
              "             1442328323415015002  2.459197e+06    1  19.782700  0.172808      g\n",
              "             1444285643415015001  2.459199e+06    2  19.927372  0.209740      R\n",
              "             1449226053415015000  2.459204e+06    2  19.965925  0.170471      R"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu0dbKbbpJLj"
      },
      "source": [
        "If you want to iterate over individual objects instead of loading all the data at once, `query_objects()` can return a generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEBGXlyM_P33"
      },
      "source": [
        "# Option 2: Get a generator that yields a DataFrame for each objectId\n",
        "\n",
        "iterator = True\n",
        "objects = pgb.bigquery.query_objects(columns, \n",
        "                                     objectIds=objectIds, \n",
        "                                     iterator=iterator,\n",
        "                                     dry_run=dry_run\n",
        "                                     )\n",
        "                                     # cleaned of duplicates\n",
        "\n",
        "for lc_df in objects:\n",
        "    print(f'\\nobjectId: {lc_df.objectId}')  # objectId in metadata\n",
        "    print(lc_df.sample(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW-xTS7vqW12"
      },
      "source": [
        "Each DataFrame contains data on a single object, and is indexed by `candid`. The `objectId` is in the metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v0F-SgwrlEm"
      },
      "source": [
        "`query_objects()` can also return a json formatted string of the query results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOtHmRP7_Pvm"
      },
      "source": [
        "# Option 3: Get a single json string with all the results\n",
        "\n",
        "format = 'json'\n",
        "lcsjson = pgb.bigquery.query_objects(columns, \n",
        "                                     objectIds=objectIds, \n",
        "                                     format=format,\n",
        "                                     dry_run=dry_run\n",
        "                                     )\n",
        "                                     # cleaned of duplicates\n",
        "print(lcsjson)\n",
        "\n",
        "# read it back in\n",
        "df = pd.read_json(lcsjson)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lumcgl6_RsY"
      },
      "source": [
        "# Option 4: Get a generator that yields a json string for a single objectId\n",
        "\n",
        "format = 'json'\n",
        "iterator = True\n",
        "jobj = pgb.bigquery.query_objects(columns, \n",
        "                                  objectIds=objectIds, \n",
        "                                  format=format, \n",
        "                                  iterator=iterator,\n",
        "                                  dry_run=dry_run\n",
        "                                  )\n",
        "                                  # cleaned of duplicates\n",
        "\n",
        "for lcjson in jobj:\n",
        "    print(lcjson)\n",
        "    # lc_df = pd.read_json(lcjson)  # read back to a df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNSyjMGArw3k"
      },
      "source": [
        "Finally, `query_objects()` can return the raw query job object that it gets from its API call using `google.cloud.bigquery`'s `query()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsY_T60i8YjT"
      },
      "source": [
        "# Option 5: Get the `query_job` object\n",
        "#           (see the section on using google.cloud.bigquery directly)\n",
        "\n",
        "query_job = pgb.bigquery.query_objects(columns, \n",
        "                                       objectIds=objectIds, \n",
        "                                       format='query_job',\n",
        "                                       dry_run=dry_run\n",
        "                                       )\n",
        "# query_job is iterable\n",
        "# each element contains the aggregated history for a single objectId\n",
        "# Beware: this has not been cleaned of duplicate entries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNwVq3kkpf7t"
      },
      "source": [
        "# Option 5 continued: parse query_job results row by row\n",
        "\n",
        "for row in query_job:\n",
        "    # values can be accessed by field name or index\n",
        "    print(f\"objectId={row[0]}, magpsf={row['magpsf']}\")\n",
        "\n",
        "    # pgb can cast to a DataFrame or json string\n",
        "    # this option also cleans the duplicates\n",
        "    lc_df = pgb.bigquery.format_history_query_results(row=row)\n",
        "    print(f'\\nobjectId: {lc_df.objectId}')  # objectId in metadata\n",
        "    print(lc_df.head(1))\n",
        "    lcjson = pgb.bigquery.format_history_query_results(row=row, format='json')\n",
        "    print('\\n', lcjson)\n",
        "\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmeLC30rPwKF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLEcUP3jtVh9"
      },
      "source": [
        "#### Plot a lightcurve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqJAkLgxBO7H"
      },
      "source": [
        "# Get an object's lightcurve DataFrame with the minimum required columns\n",
        "columns = ['jd','fid','magpsf','sigmapsf','diffmaglim']\n",
        "objectId = 'ZTF20acqgklx'\n",
        "lc_df = pgb.bigquery.query_objects(columns, objectIds=[objectId], dry_run=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5-hdnhcQtZ42",
        "outputId": "e62ed752-a2ff-4f64-d0fc-e1c51c8aadf1"
      },
      "source": [
        "# make the plot\n",
        "pgb.figures.plot_lightcurve(lc_df, objectId=objectId)\n",
        "# this function was adapted from:\n",
        "# https://github.com/ZwickyTransientFacility/ztf-avro-alert/blob/master/notebooks/Filtering_alerts.ipynb"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcVZ3/8feHJQkO0GwNIklIVFAChlYiEBVsiALiAooyZhSIgPwQGQGXAZUZcBwdxQVhFBlkCUgMgwIDKqgYaRFscBKIgQSQJZFsQmRpQAwJ9Pf3xz0NRaWquypdt6qr6vN6nnr61rnn3jq3KqlvneWeo4jAzMysUhs0ugBmZtZcHDjMzKwqDhxmZlYVBw4zM6uKA4eZmVXFgcPMzKriwGEjgqSZkv5jkP3PSHp1ncvULWlZPV+zFUiaIemWavdZ83DgsKYQEZtGxEPDOYekHknHFqWFpNcOr3Qg6QspuBU+/pbOf2RRer+kvxc8/4ikMyWtLcr3L+nc35R0v6SnJd0r6cii1+6SNE/Ss+lv13Cvx2wwDhxmNRARX03B7cUHcDawCLiqKP1h4L0FabPSaf6n6BxnpfS/Ae8FOoCjgHMkvQVA0ijgWuByYEvgUuDalG6WCwcOqxtJu6Rf/U9KWijpfUVZtpF0Y/pl/VtJOxYc+2LNQNLo9Cv8YUmPSDpf0iYFeQ+RNF/SU5IelHSQpK8A+wDfTb/mv1uifJukJrMnJC0C3jyMaz0Y+BTwwYj42/qeByAizoiIeyOiPyJuB34HTE27u4GNgO9ExHMRcS4gYP9UjndLujO9F0slnVlUzrdJ+n36TJZKmpHSt5Z0XTruD5K+XNjEJOkASfdJ6pN0Xvq8XlabK8j7DUm3SOooSn+LpL9KGpee757e+9cP5/2y/DlwWF1I2hj4KfArYFvgn4FZkl5XkO0jwJeBbYD5wKzi8yRfA3YGuoDXAjsA/5ZeZ0/gMuBzwBbAvsCSiPgi2RfuienX/IklznsG8Jr0OJDs133hNZwn6bwKrnUC8EPg4xFxz1D5q5EC5JuBhSlpV2BBvHzuoAUpHbLaypFk78W7gU9IOjSda0fgBuC/gE6y93N+Ou57wGpge+Do9BgowzbAT4DPA1sD9wFvKVHWDST9AJgMHBARfYX7I+L3wH8Dl6bruhz414i4t7p3xerNgcPqZW9gU+BrEbEmIn4D/AyYXpDn5xFxc0Q8B3wRmDrwa3SAJAHHAadExOMR8TTwVeDDKcsxwMURcWP6hb68ii+iw4GvpPMuBc4t3BkRJ0TECYOdQNJosi/VWRHxPxW+7ouvn375DzxeVSLP+cAfgV+m55sCfUV5+oDNUpl7IuKu9F4sAGYDb0/5/gn4dUTMjoi1EfFYRMyXtCFwGPBvEfG3iLibrAlswMHAwoi4OiKeJ3uf/lJUho3Ta21F1iz3bJlrPpOsCe4PwHKygGUj3EaNLoC1jVcBSyOivyDtz2S1hQFLBzYi4hlJjw8cV5CnE3gFMC+LIUDWNLNh2h4HXD+cMhaVr1rnAGuBz6zHsVdGxEfL7ZT0DWA3YL+CGsYzwOZFWTcHnk7H7EVWQ9sNGAWMBn6c8o0DHizxUp1k3w3l3ouXvU8REVp39Nlrgd2BPSNiTblrioi1kmaSBZ9PF9WcbIRyjcPqZQUwTlLhv7nxZL8yB7xYu5C0Kdmv1RVF5/kr8Hdg14jYIj06UqczZF9orylThqG+lFYWliGVr2KSjiD7pX54RKyt5tgKzv0l4F1kTT5PFexaCExWQRQlaxoaaMr6EXAdMC4iOshqLAN5y71Xq4DnKf9erATGFpRNhc+Te4CPATcUNUcWX9cOZE2ElwDfSjU2G+EcOKxebgeeBf5F0saSuslGCl1RkOfg1Fk7iqyv47bUZPSiVGP5AXC2pG0h+/KRdGDKchHwMUnTUhv7DgWdrY8Ag90LciXweUlbShpL1g9TEUm7AecBHyku83BJ+jxZs9I7IuKxot09wAvAp9KggYG+m9+kv5sBj0fE6tT/808Fx84C3iHpcEkbpQ7xroh4AbgaOFPSKyRN4uX9PT8H3iDpUEkbAZ8EXllc7oiYDXwB+LWkdQJUCjgzyT6zY8gC0pcrfFusgRw4rC5Sc8V7yX41/5XsS/bIov6HH5H9+nwc2AMo12xzKvAAcJukp4BfA69Lr/MHsl+6Z5O19f8WGBiddQ7wwTRy59x1zgpfImuSWUzWif/Dwp1p9Nb5Zcr0aeAfgKu17v0cXyhzTKW+SvaL/4Hic6b39VCyDvAnyTqxDy1oHjoB+HdJT5MNILhy4KQR8TBZf8VnyN7z+WTNSwAnkvWf/IXsy/2SguP+CnwIOAt4DJgEzAWeKy54RFwK/DvwmzRooNCnyAZK/GtqovoYWdDfp6p3x+pOblK0kS41b70A7Ji+7KzO0jDdYyPibSX2bQAsI6tt3VTvsln9ucZhzWA3sqGhxSN3rEEkHShpi9Qn8QWyfpPbGlwsqxMHDhvRJB0G3AScOtjoHKu7qWQjsv5K1gR5aET8vbFFsnpxU5WZmVXFNQ4zM6tKW9wAuM0228SECRMaXQwzs6Yyb968v0ZEZ3F6WwSOCRMmMHfu3EYXw8ysqUgqOXuCm6rMzKwqDhxmZlYVBw4zM6tKW/RxmJkN19q1a1m2bBmrV69udFFqbsyYMYwdO5aNN964ovy5Bg5JFwPvAR6NiN1SWhfZDJ1jyGbgPCHNL1R87AvAXenpwxHxvpQ+kWxivK2BecARvjHMzPK2bNkyNttsMyZMmMDLJyNubhHBY489xrJly5g4cWJFx+TdVDUTOKgo7SzgSxHRRTbp2lnFByV/j4iu9ChcYvTrwNkR8VrgCbJZNc3McrV69Wq23nrrlgoaAJLYeuutq6pJ5Ro4IuJmslk3X5bMSwvPdLDuegtlpWmY9ydbYQ2yVckOHWYxzcwq0mpBY0C119WIPo6TgV9K+iZZ4FpnreJkjKS5ZM1ZX4uI/yVrnnoyLVcJ2YycO5Q6WNJxZEuMMn58VevxmJnVRPfMbgB6ZvQ0tBy11ohRVZ8gWy96HHAK2SIupewYEVPIFp75TqmFYAYTERdExJSImNLZuc6Nj9bMuruzRzNq5rJbwx199NFsu+227Lbbbg0tRyMCx1Fkq4tBtvbxnqUyRcTy9PchslXO3ki2aMwWadUxyJarXF7qeDOzVjNjxgx+8YtfNLoYDQkcK4C3p+39gfuLM6SlO0en7W2AtwKL0iphNwEfTFmPAq7NvcRmZuuh77k+Hu57mN6lvTU537777stWW21Vcf5nn32Www8/nEmTJvH+97+fvfbaqybTL+U9HHc20A1sI2kZ2bKgHwfOSbWG1aR+CElTgOMj4lhgF+C/JfWTBbevRcSidNpTgSsk/QdwJ+WbuszMGqZ3aS8LHllAf/Qz7bJpzDlyDlPHTc3ltc4/P1vR+Pjjj39Z+nnnnceWW27JokWLuPvuu+nq6qrJ6+UaOCJieplde5TIOxc4Nm3/HnhDmXM+RJnmLTOzkaJnSQ/90Q/AmhfW0LOkJ7fAURwwBtxyyy2cdNJJAOy2225Mnjy5Jq/nKUfMzHLQPaGbDZR9xY7acBTdE7obW6AacuAwM8vB1HFTmbzdZCZuMTHXZqrBvPWtb+XKK68EYNGiRdx1111DHFEZBw4zs5x0jO5gfMf4mgWN6dOnM3XqVO677z7Gjh3LRRdlXbznn3/+i/0chU444QRWrVrFpEmTOP3009l1113p6OgYdjk8yaGZWZOYPXt2yfRyfRxjxozh8ssvZ8yYMTz44IO84x3vYMcddxx2ORw4zMxy0ug7xp999ln2228/1q5dS0Rw3nnnMWrUqGGf14HDzKxFbbbZZrksm+0+DjMzq4oDh5mZVcWBw8zMquLAYVZPfX3w8MPQW5u5i2yEa9HZkB04zOqltxcWLIDFi2HaNAcPq8rSpUvZb7/9mDRpErvuuivnnHPOkMf09PTQ0dFBV1cXr3/96/nsZz9bk7I4cJjVS08P9GdzF7FmTfbcrEIbbbQR3/rWt1i0aBG33XYb3/ve91i0aNGQx+2zzz7Mnz+fO++8k5/97Gfceuutwy6LA4dZvXR3wwbpv9yoUS3ZhGFFatg0uf322/OmN70JyIbZ7rLLLixfXvlyRJtssgldXV1VHVOOA4dZvUydCpMnw8SJMGdO9txaV45Nk0uWLOHOO+9kr732AspPOVLoiSee4P7772ffffcd9us7cFjzaeYO5o4OGD/eQaMd5NQ0+cwzz3DYYYfxne98h8033xzIphwpN+3I7373O3bffXd22GEHDjzwQF75ylcOuwwOHNZc3MFszSKHpsm1a9dy2GGH8ZGPfIQPfOADFR2zzz778Mc//pGFCxdy0UUXMX/+/GGXw4HDmos7mK1Z1LhpMiI45phj2GWXXfj0pz9d9fETJ07ktNNO4+tf//qwygEOHNZs3MFszaSGTZO33norP/zhD/nNb35DV1cXXV1dXH/99UBlfRyQNWndfPPNLFmyZFhlyW2SQ0kXA+8BHo2I3VJaF3A+MAZ4HjghIv5QdNx+wNkFSa8HPhwR/ytpJvB2oC/tmxERw693WfMY+BXX1wezZrmvwNrG2972NiKi5L5y/Rvd3d10F/y42mSTTWoyqirP2XFnAt8FLitIOwv4UkTcIOng9Ly78KCIuAnoApC0FfAA8KuCLJ+LiJ/kV2wb8To6ssdgQWPgP0srN2W1wzU2uxb9bHJrqoqIm4HHi5OBzdN2B7BiiNN8ELghIp6tcfGsXbXoFBDraJfrtIaodx/HycA3JC0Fvgl8foj8HwaKl7z6iqQFks6WNLrcgZKOkzRX0txVq1YNr9RmZlC2qajZVXtd9Q4cnwBOiYhxwCnAReUyStoeeAPwy4Lkz5P1ebwZ2Ao4tdzxEXFBREyJiCmdnZ21KLuZtbExY8bw2GOPtVzwiAgee+wxxowZU/Ex9V4B8CjgpLT9Y+DCQfIeDlwTEWsHEiJiZdp8TtIlQG1m7DIzG8LYsWNZtmwZrdiCMWbMGMaOHVtx/noHjhVko6J6gP2B+wfJO52ipixJ20fESkkCDgXuzqmcZmYvs/HGGzNx4sRGF2NEyHM47myyEVPbSFoGnAF8HDhH0kbAauC4lHcKcHxEHJueTwDGAb8tOu0sSZ2AgPlA6TFoZmaWm9wCR0RML7NrjxJ55wLHFjxfAuxQIt/+tSqfjXAeamo2YvnOcWtNI3UixJ6e2gXDkXqN1vIcOKyx8rjfoB0mQmyHa7QRy4HDWs9gEyG2yq90T/ZoDeTAYa2n3ESIrfQrfajJHlslQNqI5MBhrafcdNat9Ct9sCm7ax0gPX2JFXHgsNZUajrrVpuSvdyU3a0UIG1EcuCw9tEua363WoC0EceBw0amvNro22HN78ECpJudrAYcOGzkaaVO7EZphwBpDVPvuarMhlaqjb7wC9Bt9mYN5RqHjTxuo29ubg5reQ4cVh/VfJm0Sye2WZNyU5WNTJWsKz4YN2eZ5caBw6xZOThag7ipqh25DdrMhsGBwxrLcyrVl99vqwEHDmucRtyvUcv1MJqN57CyGnHgsMbxnEr15ffbasSd49Y4A/dr9Pf7fo1aKxUU6vV+9/Vlj95eD6VuUbnWOCRdLOlRSXcXpO0uqVfSXZJ+KmnzMsceJOk+SQ9IOq0gfaKk21P6/0galec1WI58v0Z91eP99nQxbSHvpqqZwEFFaRcCp0XEG4BrgM8VHyRpQ+B7wLuAScB0SZPS7q8DZ0fEa4EngGPyKbrVxWBTg7sppfbynsPKzWFtIdfAERE3A48XJe8M3Jy2bwQOK3HonsADEfFQRKwBrgAOkSRgf+AnKd+lwKE1L7iZvaSakVieLqYtNKJzfCFwSNr+EDCuRJ4dgKUFz5eltK2BJyPi+aL0dUg6TtJcSXNXrVpVk4LbMHgYaHOqtunJzY9toRGB42jgBEnzgM2ANXm8SERcEBFTImJKZ2dnHi9hlXK7d/Nan6YnT+ne8uoeOCLi3og4ICL2AGYDD5bItpyX10TGprTHgC0kbVSUbiOZ272bl5uerIS6Bw5J26a/GwCnA+eXyPZ/wE5pBNUo4MPAdRERwE3AB1O+o4Br8y+1DYu/fJqXm56shLyH484GeoHXSVom6RiyEVJ/Au4FVgCXpLyvknQ9QOrDOBH4JXAPcGVELEynPRX4tKQHyPo8LsrzGqxy3TO76Z7Zve6OEfTlU7aMVp6bnqxIrjcARsT0MrvOKZF3BXBwwfPrgetL5HuIbNSVNZPhTpNuteFmQqsBTzliZmZVceBodp5ozhrFQ6zblgOHmVWvXYZY+4dZSZ7k0Bpq/l/mA9DV4HJYlUoNsR7ov3I/SstzjaMduYnBhstDrNuaA0e7aZcmBsvXCBpibfXnpqp2M1gTQwOcfFrWSNXTsBLYevMQ67blGke7cRODmQ2TA0e7cRODmQ2Tm6raUQOaGDx6ytrGQC2+hUeXOXBYXdS7L2NgPqqeGS9/xb7n+uhb3Ufv0l6mjnNty2x9uKnK2kbv0l4WPLKAxU8uZtpl0+hd6hFlZuvDgcPaRs+SHvojG1G25oU19CzpaWyBzJqUA4eNSHlMf949oZsNlP2TH7XhKLon1Pb81oJ8s2xJDhzWUH3P9fFw38N1aTaaOm4qk7ebzMQtJjLnyDnu46hUT09Ld/SW5Ztly6oocCjzUUn/lp6Pl+Q1MWxYGtHn0DG6g/Ed4x00bGhe8risSmsc5wFTgYGFmZ4GvpdLiaxtuM/BRrTBbpZt81lzKw0ce0XEJ4HVABHxBDAqt1JZU6q22cl9Djai+WbZsioNHGslbQgEgKROoD+3UlnTWZ9mJ/c52IjX7Out51QzqjRwnAtcA2wr6SvALcBXBztA0sWSHpV0d0Ha7pJ6Jd0l6aeSNi9x3DhJN0laJGmhpJMK9p0pabmk+elxcPHxbWeEjPoYqtmpXG3EfQ5NrF07za2ywBERs4B/Af4TWAkcGhE/HuKwmcBBRWkXAqdFxBvIAtHnShz3PPCZiJgE7A18UtKkgv1nR0RXelxfSflb1gga9TFYs5NvvLO2Uu7HXAv1iwwaOCRtNfAAHgVmAz8CHklpZUXEzcDjRck7Azen7RuBw0octzIi7kjbTwP3ADtUcC3tZwSN+his2cmd4NY2RtCPuTwNVeOYB8xNf1cBfwLuT9vz1uP1FgKHpO0PAeMGyyxpAvBG4PaC5BMlLUhNYVsOcuxxkuZKmrtq1ar1KGoTGGFTpJdrdmpEJ3g97w8xe1Ejfsw1oCYzaOCIiIkR8Wrg18B7I2KbiNgaeA/wq/V4vaOBEyTNAzYD1pTLKGlT4Crg5Ih4KiV/H3gN2SSrK4FvDVL2CyJiSkRM6ezsXI+iNoEmGfVR705wN41Zw6zPj7kmbMKqtHN878L+hIi4AXhLtS8WEfdGxAERsQdZs9eDpfJJ2pgsaMyKiKsLjn8kIl6IiH7gB4BvQmySUR/17AR305g1TJP8mBuuSqdVXyHpdODy9PwjwIpqX0zSthHxqKQNgNOB80vkEXARcE9EfLto3/YRsTI9fT9wd/HxVoEWHwkz0DTWH/2+P8Ty0deXPXp71w0ObbCkbqU1julAJ9lIqGuAbXnpLvKSJM0GeoHXSVom6RhguqQ/AfeSBZ5LUt5XSRqo0bwVOALYv8Sw27PSUN4FwH7AKZVeqDWX4fRRDNpRP6NnnTU6zKrSJh3gg6moxhERjwMnDZnx5ceUCyznlMi7Ajg4bd8CqMw5j6imDNacBvoo+qOfaZdNW69+kY7RHXSM7vD9IVZ7pTrAW7h2UUpFgUPSTaS7xgtFxP41L5G1lVK//kv1UTgAWEOUatYd6ADv7x8RoxkbodI+js8WbI8hu//i+doXx8x9FDbCDXSA9/XBrFltV9uAypuqiu/ZuFXSH3Ioj9mLfRR9q/uY9YFZrm3YyNMGHeCDqbSpqvAu8Q2APYCOXEpkhvsorAUNNhKryVTaVDWPrI9DZE1Ui4Fj8iqUmVlLGRiJ1d+fjcRq8ns8Kg0cu0TE6sIESaNzKI+1KA+BrZ+Btdr9no8gjRqJlVMtp9L7OH5fIq39Bi+bma2PRswrl+P9JoPWOCS9kmxm2k0kvZGX7q/YHHhFzUphZtYqSg3hHWwkVl59HznWcoZqqjoQmAGMBQqn/3ga+EJNSmBm1g5KjcTKs+8jx/tNBg0cEXEpcKmkwyLiqpq9qplZM6vVfG959n3keL/JUE1VH42Iy4EJkj5dvL94EkIzM6tC3neh53S/yVBNVf+Q/m5a01c1y5lHFFlTqEWtoAH3hwzVVPXf6e+X6lIaa2r+sjZbD8OpFTTo/pBK7xzvBD4OTCg8JiKOzqdYVrEWX1vDzAbRoPtDKr0B8Frgd2RLyL6QX3HMMq69mFWgQTP1Vho4XhERp+ZaEjMzq06DZuqt9M7xnxWswmdmI9hwVk+0JtTRAePH13Xuq0oDx0lkwePvkp6S9LSkp/IsmJlVb2D1xMVPLmbaZdMcPCwXFQWOiNgsIjaIiE0iYvP0fPPBjpF0saRHJd1dkLa7pN60bvhPJZU8h6QlKc98SXML0reSdKOk+9PfLSu9ULN2UGr1RLNaqyhwSHpTicdrJA3WRzITOKgo7ULgtIh4A3AN8LlBjt8vIroiYkpB2mnAnIjYCZiTnptZMrB6IuDVE0eanp6WGQVZaVPVecBtwA/S4zbgx8B9kg4odUBE3Aw8XpS8M3Bz2r6RbAnaahwCXJq2LwUOrfJ4s5Y2sHrixC0mMufIOV4Iy3JRaeBYAbwxIvaIiD2ALuAh4J3AWVW83kKyL3+ADwHjyuQL4FeS5kk6riB9u4hYmbb/AmxX7oUkHSdprqS5q1atqqKIZs2tY3QH4zvGO2hYbioNHDtHxMKBJxGxCHh9RDxU5esdDZwgaR6wGbCmTL63RcSbgHcBn5S0b3GGiAiyAFNSRFwQEVMiYkpnZ2eVxTQzq5MmbMKq9D6OhZK+D1yRnv8jsCitAri20heLiHuBAwAk7Qy8u0y+5envo5KuAfYka+J6RNL2EbFS0vbAo5W+tpmZ1UalNY4ZwAPAyenxUEpbC+xX6YtJ2jb93QA4HTi/RJ5/kLTZwDZZoBkYmXUdcFTaPorsjnYzMyslp9pMRTWOiPg78K30KPZMqWMkzQa6gW0kLQPOADaV9MmU5WrgkpT3VcCFEXEwWb/FNZIGyvejiPhFOuZrwJWSjgH+DBxeSfnNzKx2Kp3kcCfgP4FJwJiB9Ih4dbljImJ6mV3nlMi7Ajg4bT8E7F7mnI8B0yops5mZ5aPSpqpLgO8Dz5M1TV0GXJ5XoczMbOSqNHBsEhFzAEXEnyPiTMp0bJuZWWurdFTVc6lD+35JJwLL8aqAZmZtqZpJDl8BfArYAziCl0Y3mZlZG1F2H11rmzJlSsydO3fojGZm9iJJ84rmCwSGaKqSdN1g+yPifcMtWNMaWGmrye74NDMbrqH6OKYCS4HZwO2Aci+RmZmNaEMFjleSTWQ4Hfgn4OfA7MJ5q8zMrL0M2jkeES9ExC8i4ihgb7JpR3rSyCozM2tDQw7HTRMZvpus1jEBOJdsESYzM2tDQ3WOXwbsBlwPfCki7h4sv5mZtb6hahwfBf5Gdh/Hp9LEg5B1ksdQ646bWfvpntkNQM+MnoaWw/IzaOCIiEpvEDQzszbhwGBmZlVx4DCz9dI9s/vFZilrLw4cZmZWFQcOMzOrigOHmZlVxYHDzMyqklvgkHSxpEcl3V2QtrukXkl3SfqppHXuA5H0OknzCx5PSTo57TtT0vKCfQfnVX4gmwF3YBZcMzMD8q1xzAQOKkq7EDgtIt5ANm3J54oPioj7IqIrIrrIFo16lpdPcXL2wP6IuD6fopuZWTm5BY6IuBl4vCh5Z+DmtH0jcNgQp5kGPBgRf65x8Yavrw8efhh6extdEjOzuqp3H8dC4JC0/SFg3BD5P0y2FkihEyUtSE1hW5Y7UNJxkuZKmrtq1ar1L3Epvb2wYAEsXgzTpjl4mFlbqXfgOBo4QdI8YDNgTbmMkkYB7wN+XJD8feA1QBewEvhWueMj4oKImBIRUzo7O2tR9pf09EB/f7a9Zo1XATSztjLktOq1FBH3AgcASNqZbLr2ct4F3BERjxQc/+K2pB8AP8upqIPr7oYNNsiCx6hR7kA3s7ZS1xqHpG3T3w2A04HzB8k+naJmKknbFzx9P9CYad6nToXJk2HiRJgzJ3tuZtYm8hyOOxvoBV4naZmkY4Dpkv4E3AusAC5JeV8l6fqCY/+BbMnaq4tOe1YayrsA2A84Ja/yD6mjA8aPd9Aws7aTW1NVREwvs+ucEnlXAAcXPP8bsHWJfEfUrIBmZrZefOe4mZlVxYHDzNZL33N9PNz3ML1LeytKt9bhwGFmVetd2suCRxaw+MnFTLts2otBoly6tRYHDjOrWs+SHvoju5dpzQtr6FnSM2i6tRYHDjOrWveEbjZQ9vUxasNRdE/oHjTdWktdbwA0s9YwddxUJm83mb7Vfcz6wCymjps6aLq1FgcOM1svHaM76BjdsU5wKJdurcNNVWZmVhUHjsF46nQzs3U4cJTjqdPNzEpy4CjHU6ebmZXkwFHOwNTp4KnTzcwKOHCU46nTzcxK8nDcwXR0ZI9SQcNNV2bWplzjMDOzqrjGMYj5f5kPZAucF+ue2Q1Az4yeupXHzGwkcI3DzMyq4sBhZlYH3TO7X2ypaHYOHGZmVpXcAoekcZJukrRI0kJJJ6X0rSTdKOn+9HfLMscflfLcL+mogvQ9JN0l6QFJ50pSXtdgZmbryrPG8TzwmYiYBOwNfFLSJOA0YE5E7ATMSc9fRtJWwBnAXsCewBkFAeb7wMeBndLjoByvwczK6JnR48EhbSq3wBERKyPijrT9NHAPsANwCHBpynYpcGiJww8EboyIxyPiCeBG4CBJ2wObR8RtERHAZWWONzOznNRlOK6kCcAbgduB7SJiZdr1F2C7EofsACwteL4spe2QtovTS73mccBxAOPHj1//wptZVVwLaX25d45L2hS4Cjg5Ip4q3JdqDZHH60bEBRExJSKmdHZ25vESZmZtKdfAIWljsqAxKyKuTsmPpCYn0mKU754AAApcSURBVN9HSxy6HBhX8HxsSluetovTzcysTvIcVSXgIuCeiPh2wa7rgIFRUkcB15Y4/JfAAZK2TJ3iBwC/TE1cT0naO53/yDLHm5lZTvKscbwVOALYX9L89DgY+BrwTkn3A+9Iz5E0RdKFABHxOPBl4P/S499TGsAJwIXAA8CDwA05XoOZmRXJrXM8Im4Byt1jMa1E/rnAsQXPLwYuLpNvtxoV08zMquQ7x83MrCoOHIP42CkT+cAntqJ3qdcbNzMb4MBRRu/SXhY8soDFTy5m2mXTHDzMzBIHjjJ6lvTQH/0ArHlhDT1LehpbIDOzEcKBo4zuCd1soOztGbXhKLondL9sf99zfTzc97BrImbWdhw4ypg6biqTt5vMxC0mMufIOUwd99K6427GMrN25sAxiI7RHYzvGP+yoAFuxjKz6rVSK4UDx3oYqhnLzKxQq7VSOHCsh8GasczMirVaK0VdplVvRR2jO+gY3eGgYWZDGmil6I/+lmilcOAwM8vZQCtF3+o+Zn1gVtP/4HTgMDOrg1ZqpXAfh5mZVcWBw8zMquLAYWZmVXHgMDOzqjhwmJlZVRw4zMysKrkFDknjJN0kaZGkhZJOSulbSbpR0v3p75Ylju2S1JuOWyDpHwv2zZS0uGAd8668rsHMzNaVZ43jeeAzETEJ2Bv4pKRJwGnAnIjYCZiTnhd7FjgyInYFDgK+I2mLgv2fi4iu9Jif4zWYmVmR3G4AjIiVwMq0/bSke4AdgEOA7pTtUqAHOLXo2D8VbK+Q9CjQCTyZV3lL6ZnRU8+XMzNrCnXp45A0AXgjcDuwXQoqAH8Bthvi2D2BUcCDBclfSU1YZ0saXea44yTNlTR31apVw70EMzNLcg8ckjYFrgJOjoinCvdFRAAxyLHbAz8EPhaRppaEzwOvB94MbEVRbaXg3BdExJSImNLZ2Tn8CzEzMyDnwCFpY7KgMSsirk7Jj6SAMBAYHi1z7ObAz4EvRsRtA+kRsTIyzwGXAHvmeQ1mZvZyeY6qEnARcE9EfLtg13XAUWn7KODaEseOAq4BLouInxTtGwg6Ag4F7q596c3MrJw8Z8d9K3AEcJekgZFPXwC+Blwp6Rjgz8DhAJKmAMdHxLEpbV9ga0kz0rEz0giqWZI6AQHzgeNzvIay3HFuZu0qz1FVt5B9uZcyrUT+ucCxafty4PIy592/VmU0M7PqeT0OM7M6aKVWCk85YmZmVXHgMDOzqjhwmJlZVRw4zMysKg4cZmZWFQcOMzOrigOHmZlVxYHDzMyq4sBhZmZVUTazeWuTtIpsXqy8bAP8Ncfzj3TtfP3tfO3Q3tffDte+Y0Sssy5FWwSOvEmaGxFTGl2ORmnn62/na4f2vv52vnY3VZmZWVUcOMzMrCoOHLVxQaML0GDtfP3tfO3Q3tffttfuPg4zM6uKaxxmZlYVBw4zM6uKA0eVJH1I0kJJ/Wmd9OL94yU9I+mzBWkHSbpP0gOSTqtviWun3LVLeqekeZLuSn/3L9i3R0p/QNK5ksotJzziDfbZS/p8usb7JB1YkN4Sn30xSV2SbpM0X9JcSXumdKXP+QFJCyS9qdFlzYOkf5Z0b/r3cFZBesl/By0nIvyo4gHsArwO6AGmlNj/E+DHwGfT8w2BB4FXA6OAPwKTGn0dtbx24I3Aq9L2bsDygn1/APYmW3/+BuBdjb6OHK5/UvpcRwMT0+e9YSt99iXei18NfJbAwUBPwfYN6fPeG7i90WXN4dr3A34NjE7Ptx3s30Gjy5vHw2uOVyki7gEo9cNZ0qHAYuBvBcl7Ag9ExEMpzxXAIcCi3AtbY+WuPSLuLHi6ENhE0mhgK2DziLgtHXcZcCjZF0vTGeSzPwS4IiKeAxZLeoDsc4cW+exLCGDztN0BrEjbhwCXRfZNepukLSRtHxErG1HInHwC+Fr6vImIR1N6uX8HvY0pZn7cVFUjkjYFTgW+VLRrB2BpwfNlKa1VHQbckf7z7EB2vQNa9drLfcat/NmfDHxD0lLgm8DnU3orX/OAnYF9JN0u6beS3pzS2+HaAVzjKEXSr4FXltj1xYi4tsxhZwJnR8QzTdyMv77XPnDsrsDXgQPyKFs9DOf6W81g7wUwDTglIq6SdDhwEfCOepYvT0Nc+0Zktem9gTcDV0p6dR2L13AOHCVExPr8B9gL+GDqKNsC6Je0GpgHjCvINxZYPvxS5mM9rx1JY4FrgCMj4sGUvJzsegeM6GuH9b7+5ZT/jJvmsy822HuRmh1PSk9/DFyYtgd7L5rGENf+CeDq1Bz3B0n9ZBMetsS1V8JNVTUSEftExISImAB8B/hqRHwX+D9gJ0kTJY0CPgxc18Ci1pykLYCfA6dFxK0D6ald+ylJe6fRVEcCrfir/Trgw5JGS5oI7EQ2KKCVP/sVwNvT9v7A/Wn7OuDINLpqb6Cvxfo3AP6XrIMcSTuTDXz4K+X/HbSeRvfON9sDeD9Z2+VzwCPAL0vkOZM0qio9Pxj4E9koiy82+hpqfe3A6WQDAuYXPAZGmkwB7k7X/l3SbAXN+BjssydrwngQuI+CkWOt8tmXeC/eRlab/iNwO7BHShfwvXS9d1Fi5GGzP8gCxeXp3/UdwP5D/TtotYenHDEzs6q4qcrMzKriwGFmZlVx4DAzs6o4cJiZWVUcOMzMrCoOHGYVkPRCmgl2oaQ/SvqMpFz//6QZaEPSQXm+jlm1HDjMKvP3iOiKiF2BdwLvAs7I+TWnA7ekv2YjhgOHWZUimw31OODEdIf0BEm/k3RHerwFsmk50ozJpOezJB0iaVdJf0g1mAWSdip+jXSn/YeAGcA7JY0p2Pevab2HWyTNVlr7pWCNjAWSrpG0Zb7vhLUrBw6z9RDZVOkbAtsCjwLvjIg3Af8InJuyXUT2xY+kDuAtZFOzHA+cExFdZHfWL2NdbwEWRzbvVw/w7nSeN5PNQLw7Wa2ncEGpy4BTI2Iy2V3bedeIrE05cJgN38bADyTdRTbh3ySAiPgt2VxVnWTNTVdFxPNk6zN8QdKpwI4R8fcS55wOXJG2r+Cl5qq3AtdGxOqIeBr4KbwYmLZIrwlwKbBvja/TDPDsuGbrJU2j/QJZbeMMsrmrdif7Mba6IOtlwEfJJjj8GEBE/EjS7WS1iOsl/b+I+E3BuTckq1UcIumLZPM/bS1ps9wvzKwCrnGYVSnVIM4HvhvZZG8dwMqI6AeOIGvCGjCTbNEjImJROv7VwEMRcS7ZbMGTi15iGrAgIsZFNuPyjsBVZJMs3gq8V9KYtHjYe9K5+4AnJO2TznEE8FvMcuAah1llNpE0n6xZ6nngh8C3077zgKskHQn8goKlgyPiEUn3kE3FPeBw4AhJa4G/AF8teq3pZGubFLoK+EREvEvSdcACslrOXUBfynMUcL6kVwAPkWo4ZrXm2XHNcpS+xO8C3pRqBbU456aRrTT5CuBm4LiIuKMW5zarhJuqzHIi6R3APcB/1SpoJBek2s8dZB3uDhpWV65xmJlZVVzjMDOzqjhwmJlZVRw4zMysKg4cZmZWFQcOMzOryv8HUwwDSJOAUb8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "305r1rCItVLU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob8VDnewmsbg"
      },
      "source": [
        "### Cone Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgUtTQlhiryF"
      },
      "source": [
        "To perform a cone search, we query for object histories and then check whether they are within the cone. \n",
        "`pgb.bigquery.cone_search()` is a convenience wrapper for this.\n",
        "\n",
        "First we set the search parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw-XtkifnNXu"
      },
      "source": [
        "center = coord.SkyCoord(76.91, 6.02, frame='icrs', unit='deg')\n",
        "radius = coord.Angle(2, unit=u.deg)\n",
        "\n",
        "columns = ['jd', 'fid', 'magpsf', 'sigmapsf']\n",
        "# 'objectId' and 'candid' will be included automatically\n",
        "# options are in the 'candidates' table\n",
        "# pgb.bigquery.get_table_info('candidates')\n",
        "dry_run = False\n",
        "\n",
        "# we'll restrict to a handful of objects to reduce runtime, but this is optional\n",
        "objectIds = ['ZTF18aczuwfe', 'ZTF18aczvqcr', 'ZTF20acqgklx', 'ZTF18acexdlh']"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJEFLK2IuWXQ"
      },
      "source": [
        "`cone_search()` has similar options to `query_objects()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SReMyuJXnOMA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "80d65ffe-b4b1-4525-9d2f-da646e448083"
      },
      "source": [
        "# Option 1: Get a single df of all objects in the cone\n",
        "\n",
        "objects_in_cone = pgb.bigquery.cone_search(center, radius, columns, \n",
        "                                           objectIds=objectIds, \n",
        "                                           dry_run=dry_run\n",
        "                                           )\n",
        "objects_in_cone.sample(5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>sigmapsf</th>\n",
              "      <th>ra</th>\n",
              "      <th>jd</th>\n",
              "      <th>magpsf</th>\n",
              "      <th>fid</th>\n",
              "      <th>dec</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>objectId</th>\n",
              "      <th>candid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ZTF18aczuwfe</th>\n",
              "      <th>1477168703415015002</th>\n",
              "      <td>0.211510</td>\n",
              "      <td>77.496707</td>\n",
              "      <td>2.459232e+06</td>\n",
              "      <td>19.468252</td>\n",
              "      <td>2</td>\n",
              "      <td>5.387562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">ZTF20acqgklx</th>\n",
              "      <th>1444320905215015016</th>\n",
              "      <td>0.131236</td>\n",
              "      <td>76.633648</td>\n",
              "      <td>2.459199e+06</td>\n",
              "      <td>19.305840</td>\n",
              "      <td>1</td>\n",
              "      <td>7.760110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1442282165215015017</th>\n",
              "      <td>0.146518</td>\n",
              "      <td>76.633643</td>\n",
              "      <td>2.459197e+06</td>\n",
              "      <td>18.957773</td>\n",
              "      <td>2</td>\n",
              "      <td>7.760036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZTF18aczuwfe</th>\n",
              "      <th>1477214243415015002</th>\n",
              "      <td>0.200380</td>\n",
              "      <td>77.496838</td>\n",
              "      <td>2.459232e+06</td>\n",
              "      <td>19.980097</td>\n",
              "      <td>1</td>\n",
              "      <td>5.387240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZTF20acqgklx</th>\n",
              "      <th>1416400905215015017</th>\n",
              "      <td>0.163898</td>\n",
              "      <td>76.633719</td>\n",
              "      <td>2.459171e+06</td>\n",
              "      <td>19.761951</td>\n",
              "      <td>2</td>\n",
              "      <td>7.760113</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  sigmapsf         ra  ...  fid       dec\n",
              "objectId     candid                                    ...               \n",
              "ZTF18aczuwfe 1477168703415015002  0.211510  77.496707  ...    2  5.387562\n",
              "ZTF20acqgklx 1444320905215015016  0.131236  76.633648  ...    1  7.760110\n",
              "             1442282165215015017  0.146518  76.633643  ...    2  7.760036\n",
              "ZTF18aczuwfe 1477214243415015002  0.200380  77.496838  ...    1  5.387240\n",
              "ZTF20acqgklx 1416400905215015017  0.163898  76.633719  ...    2  7.760113\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiXoz826nODh"
      },
      "source": [
        "# Option 2: Get a single json string of all objects in the cone\n",
        "format = 'json'\n",
        "\n",
        "objects_in_cone = pgb.bigquery.cone_search(center, radius, columns, \n",
        "                                           objectIds=objectIds, \n",
        "                                           format=format, \n",
        "                                           dry_run=dry_run\n",
        "                                           )\n",
        "objects_in_cone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY3IO_3EzewP"
      },
      "source": [
        "# Option 3: Get a generator that yields dfs of individual objects in the cone\n",
        "iterator = True\n",
        "\n",
        "objects_in_cone = pgb.bigquery.cone_search(center, radius, columns, \n",
        "                                           objectIds=objectIds, \n",
        "                                           iterator=iterator, \n",
        "                                           dry_run=dry_run\n",
        "                                           )\n",
        "for obj in objects_in_cone:\n",
        "    print(f'objectId: {obj.objectId}')  # objectId in metadata\n",
        "    print(obj.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFyTS8Jzem2"
      },
      "source": [
        "# Get a generator that yields a json string of individual objects in the cone\n",
        "format = 'json'\n",
        "iterator = True\n",
        "\n",
        "objects_in_cone = pgb.bigquery.cone_search(center, radius, columns, \n",
        "                                           objectIds=objectIds, \n",
        "                                           format=format, \n",
        "                                           iterator=iterator, \n",
        "                                           dry_run=dry_run\n",
        "                                           )\n",
        "for obj in objects_in_cone:\n",
        "    print(obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrMNVezhjqEy"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1fk4u06Bcq"
      },
      "source": [
        "### Direct access using `google.cloud.bigquery`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyk8BL3URAsY"
      },
      "source": [
        "The previous sections demonstrated convenience wrappers for querying with `google.cloud.bigquery`. Here we demonstrate using these tools directly with some basic examples. View `pgb_utils` source code for more examples.\n",
        "\n",
        "Links to more information:\n",
        "- [Query syntax in Standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax)\n",
        "- [`google.cloud.bigquery` docs](https://googleapis.dev/python/bigquery/latest/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_kyHjzdtFpD"
      },
      "source": [
        "Query setup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzfwhW2iQvrG"
      },
      "source": [
        "# Create a BigQuery Client to handle the connections\n",
        "bq_client = bigquery.Client(project=my_project_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv5lFH3aQvrG"
      },
      "source": [
        "# Write the standard SQL query statement\n",
        "\n",
        "# pgb.bigquery.get_dataset_table_names()  # view available tables\n",
        "# pgb.bigquery.get_table_info('<table>')  # view available column names\n",
        "\n",
        "# construct the full table name\n",
        "table = 'salt2'\n",
        "dataset = 'ztf_alerts'\n",
        "full_table_name = f'{pgb_project_id}.{dataset}.{table}'\n",
        "\n",
        "# construct the query\n",
        "query = (\n",
        "    f'SELECT objectId, candid, t0, x0, x1, c, chisq, ndof '\n",
        "    f'FROM `{full_table_name}` '\n",
        "    f'WHERE ndof>0 and chisq/ndof<2 '\n",
        ")\n",
        "\n",
        "# note: if you want to query object histories you can get the\n",
        "# query statement using `pgb.bigquery.object_history_sql_statement()`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq2DeGipQvrF"
      },
      "source": [
        "# Let's create a function to execute a \"dry run\"\n",
        "# and tell us how much data will be processed.\n",
        "# This is essentially `pgb.bigquery.dry_run()`\n",
        "def dry_run(query):\n",
        "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
        "    query_job = bq_client.query(query, job_config=job_config)\n",
        "    nbytes, TB = query_job.total_bytes_processed, 1e12\n",
        "    print(f'\\nQuery statement:')\n",
        "    print(f'\\n\"{query}\"\\n')\n",
        "    print(f'will process {nbytes} bytes of data.')\n",
        "    print(f'({nbytes/TB*100:.3}% of your 1 TB Free Tier monthly allotment.)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz6-_tYYQvrG"
      },
      "source": [
        "# Find out how much data will be processed\n",
        "dry_run(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j117iwvXQvrG"
      },
      "source": [
        "Query:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeE4LYFnQvrH"
      },
      "source": [
        "# Make the API request\n",
        "query_job = bq_client.query(query)\n",
        "# Beware: the results may contain duplicate entries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VIyok0pQvrH"
      },
      "source": [
        "Format and view results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CWQnJXVQvrH"
      },
      "source": [
        "# Option 1: dump results to a pandas.DataFrame\n",
        "df = query_job.to_dataframe()\n",
        "\n",
        "# some things you might want to do with it\n",
        "df = df.drop_duplicates()\n",
        "df = df.set_index(['objectId','candid']).sort_index()\n",
        "\n",
        "df.hist()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VIr_awwQvrH"
      },
      "source": [
        "# Option 2: parse results row by row\n",
        "for r, row in enumerate(query_job):\n",
        "    \n",
        "    # row values can be accessed by field name or index\n",
        "    print(f\"objectId={row[0]}, t0={row['t0']}\")\n",
        "    \n",
        "    if r>5: break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq_62zT7QvrH"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nDSzXh1khgt"
      },
      "source": [
        "## 3b) Command-line tool `bq`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ZRmdYixd7x"
      },
      "source": [
        "All commands in this section are executed using the `run` function we created earlier to run commands on the command-line.\n",
        "\n",
        "`bq`'s default dialect is legacy SQL. Here we use standard SQL by passing `--use_legacy_sql=false`.\n",
        "<!-- (see [change to standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/enabling-standard-sql#changing_from_the_default_dialect)) -->\n",
        "\n",
        "Links to more information:\n",
        "- [Quickstart using the bq command-line tool](https://cloud.google.com/bigquery/docs/quickstarts/quickstart-command-line)\n",
        "- [Reference of all `bq` commands and flags](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)\n",
        "- [Query syntax in Standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaUD-I_uJV3h"
      },
      "source": [
        "# Get help\n",
        "run('bq help query')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhAlHJaPH5FY"
      },
      "source": [
        "# view the schema of a table\n",
        "run('bq show --schema --format=prettyjson ardent-cycling-243415:ztf_alerts.candidates')\n",
        "# run('bq show --schema --format=prettyjson ardent-cycling-243415:ztf_alerts.alerts')\n",
        "\n",
        "# Note: The first time you make a call with `bq` you will ask you to \n",
        "# initialize a .bigqueryrc configuration file. Follow the directions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFTI682_kAtG"
      },
      "source": [
        "# Query: dry run\n",
        "\n",
        "# first we do a dry_run by including the flag --dry_run\n",
        "bq_query = \"\"\"bq query \\\n",
        "--dry_run \\\n",
        "--use_legacy_sql=false \\\n",
        "'SELECT \n",
        "    objectId, candid, t0, x0, x1, c, chisq, ndof\n",
        "FROM \n",
        "    `ardent-cycling-243415.ztf_alerts.salt2`\n",
        "WHERE \n",
        "    ndof>0 and chisq/ndof<2\n",
        "LIMIT\n",
        "    10'\n",
        "\"\"\"\n",
        "\n",
        "run(bq_query)\n",
        "# Note: to execute this directly in the command-line, enter everything \n",
        "# contained within the triple quotes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPuGfMbPiUPN"
      },
      "source": [
        "# Query\n",
        "\n",
        "bq_query = \"\"\"bq query \\\n",
        "--use_legacy_sql=false \\\n",
        "'SELECT \n",
        "    objectId, candid, t0, x0, x1, c, chisq, ndof\n",
        "FROM \n",
        "    `ardent-cycling-243415.ztf_alerts.salt2`\n",
        "WHERE \n",
        "    ndof>0 and chisq/ndof<2\n",
        "LIMIT\n",
        "    10'\n",
        "\"\"\"\n",
        "\n",
        "run(bq_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quXBg_AIwJfo"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwxBVDcgVsOZ"
      },
      "source": [
        "# 4) Files in Cloud Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxcGxp_1WkK8"
      },
      "source": [
        "I plan to rename the files and the bucket before the workshop as follows:\n",
        "- old bucket: `ardent-cycling-243415_ztf_alert_avro_bucket`\n",
        "- new bucket: `ardent-cycling-243415_ztf_alert_avros`\n",
        "- old files: `{ztf_topic}_{kafka_timestamp}.avro`\n",
        "- new files: `{objectId}.{candid}.{ztf_topic}.avro`\n",
        "---\n",
        "\n",
        "- Pricing\n",
        "    - [Cloud Storage pricing](https://cloud.google.com/storage/pricing)\n",
        "        - [Free Tier](https://cloud.google.com/storage/pricing#cloud-storage-always-free) includes >=5000 operations per month, but is limited to US regions(*). Standard operations are $0.004-$0.05 per 10,000 operations (depending on type).\n",
        "\n",
        "(*) We are interested in feedback from users outside the US on the behavior of this section of this tutorial for you. You can send feedback to troy.raen@pitt.edu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHoxCS-EAH4J"
      },
      "source": [
        "## 4a) Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASizSGpQC_46"
      },
      "source": [
        "- [Python Client documentation](https://googleapis.dev/python/storage/latest/client.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WolBMdS2K1OV"
      },
      "source": [
        "### Download files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oti0W9vANHkI"
      },
      "source": [
        "Download alerts from a given night"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtBrGafqO7gr"
      },
      "source": [
        "night = 20210404  # yyyymmdd. ENTER YOUR DATE\n",
        "bucket_name = f'{pgb_project_id}_ztf_alert_avro_bucket'\n",
        "\n",
        "# Create a client and request a list of files\n",
        "storage_client = storage.Client(my_project_id)\n",
        "bucket = storage_client.get_bucket(bucket_name)\n",
        "blobs = bucket.list_blobs(prefix=f'ztf_{night}', delimiter='/')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keej033SNJZZ"
      },
      "source": [
        "# download the files\n",
        "for b, blob in enumerate(blobs):\n",
        "    local_path = f'{colabpath_noesc}/{blob.name}'\n",
        "    blob.download_to_filename(local_path)\n",
        "\n",
        "    if b>5:\n",
        "        break"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xz7JjM0_ow9"
      },
      "source": [
        "### Plot cutouts and lightcurves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NULKsOFDPP9G"
      },
      "source": [
        "Open a file\n",
        "(see the previous section to download files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZuY3Gf8czB"
      },
      "source": [
        "paths = Path(colabpath_noesc).glob('*.avro')\n",
        "for path in paths:\n",
        "    with open(path, 'rb') as fin:\n",
        "        alert_list = [r for r in fastavro.reader(fin)]\n",
        "    break\n",
        "alert_dict = alert_list[0]  # extract the single alert packet\n",
        "\n",
        "print(alert_dict.keys())\n",
        "print(alert_dict.candidate.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8ymT1kPPXWM"
      },
      "source": [
        "Plot cutouts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA-p8FaqOG90"
      },
      "source": [
        "pgb.figures.plot_cutouts(alert_dict)\n",
        "# this function was adapted from:\n",
        "# https://github.com/ZwickyTransientFacility/ztf-avro-alert/blob/master/notebooks/Filtering_alerts.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J8JpowcPbUu"
      },
      "source": [
        "Cast to a dataframe and plot lightcurves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULrUTnXNOG0V"
      },
      "source": [
        "dflc = pgb.utils.alert_dict_to_dataframe(alert_dict)\n",
        "pgb.figures.plot_lightcurve(dflc)\n",
        "# both functions were adapted from:\n",
        "# https://github.com/ZwickyTransientFacility/ztf-avro-alert/blob/master/notebooks/Filtering_alerts.ipynb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S8r4d6ePffd"
      },
      "source": [
        "Plot everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ok0bfCcOkw4"
      },
      "source": [
        "pgb.figures.plot_lightcurve_cutouts(alert_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-TsZDqwDC8O"
      },
      "source": [
        "## 4b) Command-line tool `gsutil`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XovgM2uZDK1Q"
      },
      "source": [
        "- [Quickstart: Using the gsutil tool](https://cloud.google.com/storage/docs/quickstart-gsutil)\n",
        "- [`gsutil cp` - Copy files and objects](https://cloud.google.com/storage/docs/gsutil/commands/cp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uak8D2ZTDKYe"
      },
      "source": [
        "# Get help\n",
        "# run('gsutil help')\n",
        "run('gsutil help cp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgjYFVY7Sh9W"
      },
      "source": [
        "Download a single file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wez3NlaP0bg"
      },
      "source": [
        "run(f'gsutil cp gs://ardent-cycling-243415_ztf_alert_avro_bucket/ztf_20210401_programid1_1617259046498.avro {colabpath}/.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj3IHjtuXGCa"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbkGy7YPV8IH"
      },
      "source": [
        "# 5) Apache Beam data pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEMBJBYKsiVW"
      },
      "source": [
        "[Apache Beam](https://beam.apache.org/) is an SDK that facilitates writing and executing data pipelines. Our Pitt-Google Broker uses Beam to run our data-processing pipelines on the ZTF alert stream. The [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/) is very useful.\n",
        "\n",
        "In this section of the tutorial, we will demonstrate and describe batch-mode Beam pipelines, and provide working examples that read from our BigQuery database and execute a pipeline with template functions that you can play with.\n",
        "\n",
        "_Note on streaming_: The cores of these pipelines (the pipeline minus the input and output functions) can also be plugged directly into an input function that reads data from our Pub/Sub streams and then executes in streaming mode, writing to a sink that accepts streaming data, like Pub/Sub or BigQuery. In this tutorial we will make some comments regarding streaming pipelines where applicable, but the details are left for a future tutorial. In the meantime, please contact us with questions.\n",
        "\n",
        "_Note on environments_: We will execute pipelines directly in Colab, but the same pipelines can be executed on a local machine or various distributed systems using different \"runners\". \n",
        "For example, see \n",
        "[Apache Flink](https://beam.apache.org/documentation/runners/flink/), \n",
        "[Apache Spark](https://beam.apache.org/documentation/runners/spark/), \n",
        "[Google Dataflow](https://beam.apache.org/documentation/runners/dataflow/), and\n",
        "[DirectRunner](https://beam.apache.org/documentation/runners/direct/) (the last being the default, which we use below).\n",
        "The user sets a few configurations and the runner does the work of executing the pipeline, managing the required environments and resources (e.g., VM's) and distributing the work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uji-PPVDugcY"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lM2UnLgY5bn"
      },
      "source": [
        "## 5a) A demo example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBZ623M1alsy"
      },
      "source": [
        "To demonstrate the basic steps of an Apache Beam pipeline, we'll create one that counts the words in Shakespeare's King Lear and then filters for low-frequency words.\n",
        "Adapted from [this notebook](https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-py.ipynb).\n",
        "\n",
        "Streaming note: [here is an example](https://beam.apache.org/documentation/sdks/python-streaming/) that modifies a similar word-counting pipeline for streaming input via Pub/Sub. Note that \"windowing\" is only required because this example aggregates elements in the pipeline (by counting the frequency of each word). This is not necessary in order to, for example, process each lightcurve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybDIwJNlecm5"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F3T60obsnxH"
      },
      "source": [
        "Setup: create some paths and download a copy of King Lear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tko8YgmY3sz"
      },
      "source": [
        "kinglearpath = f'{colabpath}/kinglear'\n",
        "kinglearpath_noesc = f'{colabpath_noesc}/kinglear'\n",
        "run(f'mkdir -p {kinglearpath}')\n",
        "run(f'mkdir -p {kinglearpath}/outputs')\n",
        "\n",
        "# download the text file\n",
        "run(f'gsutil cp gs://dataflow-samples/shakespeare/kinglear.txt {kinglearpath}/')\n",
        "\n",
        "# Colab Hint: Click the \"Files\" icon on the left to view a file browser."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zG1HEC5gfF-"
      },
      "source": [
        "Define and run the pipeline.\n",
        "\n",
        "This example will give you an idea about what it possible by demonstrating the use of several different Apache Beam functions that operate on the data in various ways.\n",
        "However, don't worry about the details here.\n",
        "Steps labeled 1, 2, and 3 in the code are described further in the following subsection. Working examples that connect to our data sources will follow.\n",
        "\n",
        "Note that some [operators are overloaded](https://stackoverflow.com/questions/43796046/explain-apache-beam-python-syntax):\n",
        "- `|` means `apply`\n",
        "- `>>` allows you to name the step with the preceeding string. It is optional. We use it here to improve readability. Various UIs like Dataflow use it in their displays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe9FYDkjbMEX"
      },
      "source": [
        "input_pattern = f'{kinglearpath_noesc}/*'\n",
        "outputs_prefix = f'{kinglearpath_noesc}/outputs/part'\n",
        "\n",
        "# 0. Instantiate a pipeline object\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (\n",
        "    # 1. Start the pipeline by piping it to a \"read\" function\n",
        "      pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText(input_pattern)\n",
        "\n",
        "    # 2. PROCESS THE DATA.\n",
        "    #    These `lambda` functions are simple examples of user-defined functions \n",
        "    #    applied to the output of the previous transformation in various ways.\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
        "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "      | 'Filter for counts < 10' >> beam.Filter(lambda x: x[1]<10)\n",
        "\n",
        "    # 3. Format and output the results\n",
        "      | 'Format results' >> beam.Map(lambda word_count: str(word_count))\n",
        "      | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPxTK0lziZCU"
      },
      "source": [
        "You have just run an Apache Beam data pipeline! Let's look at the first 20 results. Beware, there are no ordering guarantees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfgwkuFSiM5i"
      },
      "source": [
        "outputs_prefix_esc = f'{kinglearpath}/outputs/part'\n",
        "run('head -n 20 {}-00000-of-*'.format(outputs_prefix_esc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eQh7wbUjCdP"
      },
      "source": [
        "Final note: Here we have simply chained the pipeline steps together, but the output of each transformation is an immutable collection that we can assign to a variable and pipe in to multiple transforms, as in this code fragment:\n",
        "\n",
        "```python\n",
        "      king_lear = (\n",
        "          pipeline | 'Read lines' >> \n",
        "          beam.io.ReadFromText(input_pattern)\n",
        "      )\n",
        "      word_counts = (\n",
        "          king_lear | 'Count words' >>\n",
        "          <... count words ...>\n",
        "      )\n",
        "      sentence_counts = (\n",
        "          king_lear | 'Count sentences' >>\n",
        "          <... count sentences ...>\n",
        "      )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBRavSps54NY"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXGtYU4aGg5h"
      },
      "source": [
        "## 5b) Descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StLxvVKpbVtM"
      },
      "source": [
        "Here we describe 3 basic steps of Beam pipelines, and their related functions, in a way that is specific to the \"Pitt-Google working examples\" that follow. These general steps are labeled in the code of the demo example above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PqEEzq43XGO"
      },
      "source": [
        "1. __Read data__\n",
        "\n",
        "Here is a table of Beam's [Built-in I/O Transforms](https://beam.apache.org/documentation/io/built-in/). We will use one to read data from BigQuery.\n",
        "\n",
        "`ReadFromBigQuery()` ([docs](https://beam.apache.org/releases/pydoc/2.25.0/apache_beam.io.gcp.bigquery.html#apache_beam.io.gcp.bigquery.ReadFromBigQuery)):\n",
        "\n",
        "Output: \n",
        "- Elements are `google.cloud.bigquery` query rows, which we discussed in the \"BigQuery Database\" section. \n",
        "    - In our example pipelines below, we will cast these to DataFrames using a class in `pgb_utils.beam`, which is a simple wrapper for the function in `pgb_utils.bigquery` that does the type casting.\n",
        "\n",
        "Parameter options (see docs for complete list):\n",
        "- `query`: to read in the results of a SQL query, pass in the query as a string. See the \"BigQuery Database\" section of this tutorial for some examples.\n",
        "- `table`: to read in an entire table, pass in the full table name as `'PROJECT:DATASET.TABLE'`. See the \"BigQuery Database\" section of this tutorial for options.\n",
        "- `gcs_location`: the table or query results will be extracted and written to temporary Avro files to be processed by the pipeline (this happens in the background). Colab does not allow users to write these (often large) files to its system, so we use this parameter to specify a Google Cloud Storage location instead. Pitt-Google Broker is providing a bucket for this purpose _for the duration of the workshop_. It is set in the examples' configs below.\n",
        "- `use_standard_sql`: `True` to use BigQuery's standard SQL dialect. (default `False`)\n",
        "- `validate`: `True` to perform some checks prior to pipeline execution. `False` (default) to skip them.\n",
        "\n",
        "Streaming note: use `ReadFromPubSub()` ([docs](https://beam.apache.org/releases/pydoc/2.25.0/apache_beam.io.gcp.pubsub.html#apache_beam.io.gcp.pubsub.ReadFromPubSub))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEN1BoCI6H-3"
      },
      "source": [
        "2. __Process the data__\n",
        "\n",
        "We will use of 2 of the available methods, which covers many use cases. Here, we describe them. In the next section, we will provide working examples that you can use as templates. See the demo example above and the [Beam programming guide](https://beam.apache.org/documentation/programming-guide/) for options beyond these two.\n",
        "\n",
        "- [`beam.Filter()` method](https://beam.apache.org/documentation/transforms/python/elementwise/filter/): \n",
        "    - A. Define your `my_filter()` function. It should:    \n",
        "        - accept a single object from the collection defined by the previous step in the pipeline (e.g., a lightcurve dataframe)\n",
        "        - return `True` or `False`, where `False` means the object will be dropped\n",
        "    - B. Apply it using `beam.Filter(my_filter)`\n",
        "\n",
        "- [`beam.ParDo()` method](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) for generic parallel processing:\n",
        "    - A. Define your processing function. You must call it `process()`. It should:\n",
        "        - accept a single object from the input collection\n",
        "        - return a list of one or more objects (all of the same type) that will become elements in the resulting output collection\n",
        "    - B. Wrap your function in a class, `my_processing()` (call it whatever you'd like). This should be a child class of `beam.DoFn`.\n",
        "    - C. Apply it to the input collection using `beam.ParDo(my_processing())`\n",
        "\n",
        "Some things to keep in mind:\n",
        "- The output from each step in the pipeline is an imutable _collection_ of objects.\n",
        "    - When we chain our pipeline steps, we are piping the output collection from a given function to the input of the function that follows it. Alternately, we could assign each output to a variable, and then explicitely pipe it in to one or more of any functions that follow it. See the final note in the demo example above.\n",
        "- User-defined functions should accept and operate on a _single_ object (for example, the lightcurve or history associated with a unique `objectId`). \n",
        "    - Note that whether the pipeline is operating in batch or streaming mode makes no difference to user-defined functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KTwtriR3AVw"
      },
      "source": [
        "3. __Write the results__\n",
        "\n",
        "Here is a table of Beam's [Built-in I/O Transforms](https://beam.apache.org/documentation/io/built-in/). There are many. Your favorite option is likely to be supported. We will use one to simply write results to a file.\n",
        "\n",
        "`beam.io.WriteToText()` ([docs](https://beam.apache.org/releases/pydoc/2.25.0/apache_beam.io.textio.html#apache_beam.io.textio.WriteToText)):\n",
        "\n",
        "Input:\n",
        "- we can pipe in inputs of many different types (though the elements in a single input collection must all be of the same type).\n",
        "\n",
        "Parameter options (see docs for complete list):\n",
        "- `file_path_prefix`: output files will be written to a path beginning with this prefix, followed by a shard identifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s-Hol8TGvAx"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaZ4wfvY55bm"
      },
      "source": [
        "## 5c) Pitt-Google working examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRK4nQPbI4iO"
      },
      "source": [
        "In these examples, we will query the database for object histories and cast them to DataFrames, apply a filter and a processing function, and write the results to a text file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Sa3tN1FYqO"
      },
      "source": [
        "First, let's define a filter and a processing function.\n",
        "Both will take as input `lc_df`, a single object's lightcurve (or history) DataFrame.\n",
        "_The functions will work as-is, or you can use them as templates to create your own._\n",
        "\n",
        "_Colab Hint_: Right-click on one of the code cells with a function definition and select \"Copy to scratch cell\". Use the new scratch cell to change the function and experiment with the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNnbCyEVqiZq"
      },
      "source": [
        "# Filter for likely solar system objects\n",
        "\n",
        "def nearby_ssobject(lc_df):\n",
        "    \"\"\"Keep only objects that are within 5\" of a known solar system object.\n",
        "    To be called with `beam.Filter(nearby_ssobject)`.\n",
        "    \"\"\"\n",
        "\n",
        "    ssdistnr = lc_df['ssdistnr'].mean()\n",
        "    ssobject_is_near = (ssdistnr > 0) and (ssdistnr < 5)\n",
        "\n",
        "    return ssobject_is_near\n",
        "    # generally: return a bool where `True` means we keep this df, else drop it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98q5SjyKTiK9"
      },
      "source": [
        "# Processing: Calculate and return the mean magnitude per passband\n",
        "\n",
        "class calc_mean_mags(beam.DoFn):\n",
        "    \"\"\"Class that wraps our `process()` function to calculate mean magnitudes.\n",
        "    To be called with `beam.ParDo(calc_mean_mags())`.\n",
        "    \"\"\"\n",
        "\n",
        "    def process(self, lc_df):\n",
        "        \"\"\"Calculate mean magnitudes per passband.\"\"\"\n",
        "\n",
        "        meanmags = lc_df[['fid','magpsf']].groupby('fid').mean()\n",
        "\n",
        "        # we will write this to a file, so let's format it nicely\n",
        "        output = []\n",
        "        for fid, row in meanmags.iterrows():\n",
        "            output.append(f\"{lc_df.objectId},{fid},{row['magpsf']}\")\n",
        "\n",
        "        return output\n",
        "        # generally: return a list containing 0 or more elements, each of which\n",
        "        # becomes an element in the `ParDo`'s output collection."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo5dFk2Nxvx-"
      },
      "source": [
        "Now let's configure and run specific pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3yXrkBVlNKn"
      },
      "source": [
        "### Lightcurve pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d2yNNkTPRzl"
      },
      "source": [
        "Set configs\n",
        "\n",
        "See the \"BigQuery Database\" section for options in generating the query statement.\n",
        "\n",
        "Use `pgb.bigquery.get_table_info('candidates')` in a scratch cell to view options for column names.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYG-X2GUPrrR"
      },
      "source": [
        "# some keyword args for `ReadFromBigQuery()`\n",
        "columns = ['jd', 'fid', 'magpsf', 'sigmapsf', 'ssdistnr']\n",
        "limit = 2000  # just to reduce runtime\n",
        "query = pgb.bigquery.object_history_sql_statement(columns, limit=limit)  # str\n",
        "read_args = {\n",
        "    'query': query, \n",
        "    'project': my_project_id,  \n",
        "    'use_standard_sql': True, \n",
        "    'gcs_location': f'gs://{pgb_project_id}-workshop_beam_test',\n",
        "    # courtesy location for temp files, available for the workshop duration\n",
        "}\n",
        "\n",
        "# path to write the results\n",
        "outputs_prefix = f'{colabpath}/outputs/meanmags'\n",
        "beam_outputs_prefix = f'{colabpath_noesc}/outputs/meanmags'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExBgpQwD1a6Q"
      },
      "source": [
        "Define and run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjV7ayLbH7fL"
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "    (\n",
        "        pipeline\n",
        "        | 'Read BigQuery' >> beam.io.ReadFromBigQuery(**read_args)\n",
        "\n",
        "        # Extract the lightcurves into a DataFrame by applying a \n",
        "        # `ParDo` to a `pgb` helper function.\n",
        "        | 'Extract lightcurve df' >> beam.ParDo(pgb.beam.ExtractHistoryDf())\n",
        "        \n",
        "        # Apply the Filter\n",
        "        | 'Nearby SS object' >> beam.Filter(nearby_ssobject)\n",
        "\n",
        "        # Apply the processing function\n",
        "        | 'Calc mean mags' >> beam.ParDo(calc_mean_mags())\n",
        "        \n",
        "        # Write the results to a text file\n",
        "        | 'Write results' >> beam.io.WriteToText(beam_outputs_prefix)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV_kh2QZwdag"
      },
      "source": [
        "Congratulations! You just ran an Apache Beam pipeline that processes ZTF data.\n",
        "\n",
        "Let's look at the first 10 results. Remember, there are no ordering guarantees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvqRrrFc1PXL"
      },
      "source": [
        "run('head -n 10 {}-00000-of-*'.format(outputs_prefix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7F6Pa7M2RDv"
      },
      "source": [
        "### Cone Search pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSfqvQUq5Nfq"
      },
      "source": [
        "This pipeline for performing a cone search and processing the results is nearly identical to the lightcurve pipeline above. The differences are that here:\n",
        "- we need the additional columns `ra` and `dec`\n",
        "- we apply the convenience function `pgb.bigquery.object_is_in_cone()` as a filter on the results of `ReadFromBigQuery()`. We must pass a cone `center` and `radius` to the convenience function, so note the syntax below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBIOPtmf2RDz"
      },
      "source": [
        "Set configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At2e2rAS2RD0"
      },
      "source": [
        "# cone search params\n",
        "center = coord.SkyCoord(76.91, 6.02, frame='icrs', unit='deg')\n",
        "radius = coord.Angle(2, unit=u.deg)\n",
        "\n",
        "# some keyword args for `ReadFromBigQuery()`\n",
        "columns = ['jd', 'fid', 'ra', 'dec', 'magpsf', 'sigmapsf', 'ssdistnr']\n",
        "limit = 2000\n",
        "query = pgb.bigquery.object_history_sql_statement(columns, limit=limit)  # str\n",
        "read_args = {\n",
        "    'query': query, \n",
        "    'project': my_project_id,  \n",
        "    'use_standard_sql': True, \n",
        "    'gcs_location': f'gs://{pgb_project_id}-workshop_beam_test',\n",
        "    # courtesy location for temp files, available for the workshop duration\n",
        "}\n",
        "\n",
        "# path to write the results\n",
        "outputs_prefix = f'{colabpath}/outputs/conesearch'\n",
        "beam_outputs_prefix = f'{colabpath_noesc}/outputs/conesearch'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv73E5qb2RD0"
      },
      "source": [
        "Define and run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXohRptj2RD1"
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "    (\n",
        "        pipeline\n",
        "        | 'Read BigQuery' >> beam.io.ReadFromBigQuery(**read_args)\n",
        "\n",
        "        # Extract the lightcurves into a DataFrame by applying a \n",
        "        # `ParDo` to a `pgb` helper function.\n",
        "        | 'Extract Lightcurve df' >> beam.ParDo(pgb.beam.ExtractHistoryDf())\n",
        "        \n",
        "        # Apply the Filter\n",
        "        | 'Cone search filter' >> beam.Filter(pgb.bigquery.object_is_in_cone, \n",
        "                                              center, \n",
        "                                              radius\n",
        "                                              )\n",
        "\n",
        "        # Apply the processing function\n",
        "        | 'Calc mean mags' >> beam.ParDo(calc_mean_mags())\n",
        "        \n",
        "        # Write the results to a text file\n",
        "        | 'Write results' >> beam.io.WriteToText(beam_outputs_prefix)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eXZ_UZk2RD2"
      },
      "source": [
        "# Sample the first 10 results, remember there are no ordering guarantees.\n",
        "run('head -n 10 {}-00000-of-*'.format(outputs_prefix))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop-Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwvgroup/Pitt-Google-Broker/blob/u%2Ftjr%2Ftutorials/PGB_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnq45aCYshxk"
      },
      "source": [
        "# 1) Data Products\n",
        "\n",
        "Architecture figure highlighting the data products available to them\n",
        "\n",
        "Links to webpages:\n",
        "- [BQ table descriptions and schemas]\n",
        "- [Cloud Storage buckets]\n",
        "- [Pub/Sub streams]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NMjb0oOtxG7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_FJ8LH4omNX"
      },
      "source": [
        "# 2) Setup\n",
        "\n",
        "- Create GCP Project\n",
        "- Enable billing\n",
        "- Enable APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm8S4HbIQlK1"
      },
      "source": [
        "#--- Colab Setup\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()  # follow the instructions to authorize Google Cloud SDK \n",
        "# connect your file system (local or Drive)\n",
        "# example of down/uploading a file (also see file browser on left)\n",
        "\n",
        "\n",
        "#--- Cloud Shell Setup\n",
        "\n",
        "\n",
        "#--- Local machine setup\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaFC3geYYfyn"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubkZqvJuuMrR"
      },
      "source": [
        "project_id = 'ardent-cycling-243415'\n",
        "project_id_pgb = 'ardent-cycling-243415'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feaPBbMmtaVu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FrTV1deWZFN"
      },
      "source": [
        "# 3) Query a BigQuery Database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUjPP0ENkYlc"
      },
      "source": [
        "## 3a) Python\n",
        "\n",
        "Links with more info:\n",
        "- [Documentation](https://googleapis.dev/python/bigquery/latest/index.html)\n",
        "- [Colab Snippets](https://colab.research.google.com/notebooks/snippets/bigquery.ipynb#scrollTo=jl97S3vfNHdz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhhHT9YArfyd"
      },
      "source": [
        "# create a connection to BigQuery\n",
        "bq_client = bigquery.Client(project=project_id)\n",
        "\n",
        "# write the SQL query as a string\n",
        "sql = \"\"\"\n",
        "    SELECT name, SUM(number) as count\n",
        "    FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
        "    GROUP BY name\n",
        "    ORDER BY count DESC\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "# make an API request\n",
        "query_job = bq_client.query(sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUvYkm0cQS0a"
      },
      "source": [
        "# Option1: dump results to a pandas.DataFrame\n",
        "df = query_job.to_dataframe()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaLecK6YYWJI"
      },
      "source": [
        "# Option 2: parse results row by row\n",
        "print(\"The query data:\")\n",
        "for row in query_job:\n",
        "    # row values can be accessed by field name or index\n",
        "    print(f\"name={row[0]}, count={row['count']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nDSzXh1khgt"
      },
      "source": [
        "## 3b) Command-line tool `bq`\n",
        "\n",
        "Links with more info:\n",
        "- [Running queries from the bq command-line tool](https://cloud.google.com/bigquery/docs/bq-command-line-tool#running_queries_from_the) (also see other sections on this page)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q4hxE8QVrL3"
      },
      "source": [
        "\n",
        "\n",
        "```bash\n",
        "#--- retrieve query results from command line\n",
        "# (this cell is not executable)\n",
        "\n",
        "bq query --use_legacy_sql=false \\\n",
        "'SELECT\n",
        "  word,\n",
        "  SUM(word_count) AS count\n",
        "FROM\n",
        "  `bigquery-public-data`.samples.shakespeare\n",
        "WHERE\n",
        "  word LIKE \"%raisin%\"\n",
        "GROUP BY\n",
        "  word'\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qu69YBOtfkM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxcGxp_1WkK8"
      },
      "source": [
        "# 4) Download a file from Cloud Storage\n",
        "\n",
        "- grab cutouts from Cloud Storage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtBrGafqO7gr"
      },
      "source": [
        "#--- Download an alert from GCS bucket\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "bucket_name = f'{project_id_pgb}_ztf_alert_avro_bucket'\n",
        "fname_prefix = '1605062619785'  # recently ingested avro file name, get from logging\n",
        "local_dir = './testdownload'\n",
        "delimiter = '/'\n",
        "\n",
        "storage_client = storage.Client.from_service_account_json('GCPauth_pitt-google-broker-prototype-0679b75dded0.json')\n",
        "bucket = storage_client.get_bucket(bucket_name)\n",
        "blobs = bucket.list_blobs(prefix=fname_prefix, delimiter=delimiter) #List all objects that satisfy the filter.\n",
        "# Download the file to a destination\n",
        "# Iterating through for loop one by one using API call\n",
        "for blob in blobs:\n",
        "    destination_uri = '{}/{}'.format(local_dir, blob.name)\n",
        "    blob.download_to_filename(destination_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlnP_NoGvnQt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxdGsenapMZR"
      },
      "source": [
        "# 5) Listen to Pub/Sub Streams\n",
        "\n",
        "- Python\n",
        "  - unpack into dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEXE5Ek8vkkX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEMBJBYKsiVW"
      },
      "source": [
        "# 6) Filter or Process the Data (using Apache Beam)\n",
        "\n",
        "We use the [Apache Beam](https://beam.apache.org/) (Python) SDK to write the data pipeline (see also [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/)). This has the following advantages:\n",
        "\n",
        "1. Convenience: Native I/O functions for Pub/Sub, BigQuery, and Cloud Storage.\n",
        "2. Flexiblity: The same pipeline can accept streaming and batch inputs. -> Use the same pipeline to process the live stream and reprocess the database.\n",
        "3. Portability: The same pipeline can be run/executed in multiple environments (Google Cloud, AWS, local machine) via an execution \"runner\" ([Apache Flink](https://beam.apache.org/documentation/runners/flink/), [Apache Spark](https://beam.apache.org/documentation/runners/spark/), [Google Dataflow](https://beam.apache.org/documentation/runners/dataflow/), or [direct/local](https://beam.apache.org/documentation/runners/direct/)).\n",
        "\n",
        "Links with more info:\n",
        "- [Colab Snippets: Apache Beam](https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-py.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecuUx8mGwbR-"
      },
      "source": [
        "#--- Install Apache Beam (and other setup)\n",
        "\n",
        "# Run and print a shell command.\n",
        "def run(cmd):\n",
        "  print('>> {}'.format(cmd))\n",
        "  !{cmd}\n",
        "  print('')\n",
        "\n",
        "# Install apache-beam.\n",
        "run('pip install --quiet apache-beam')\n",
        "\n",
        "# Copy the input file into the local file system.\n",
        "run('mkdir -p data')\n",
        "run('gsutil cp gs://dataflow-samples/shakespeare/kinglear.txt data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVz_RyMSwUzK"
      },
      "source": [
        "#--- Minimal word count (example)\n",
        "\n",
        "import apache_beam as beam\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/*'\n",
        "outputs_prefix = 'outputs/part'\n",
        "\n",
        "# Running locally in the DirectRunner.\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (\n",
        "      pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
        "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "      | 'Format results' >> beam.Map(lambda word_count: str(word_count))\n",
        "      | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
        "  )\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "run('head -n 20 {}-00000-of-*'.format(outputs_prefix))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}